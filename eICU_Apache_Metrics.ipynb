{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EMBC2020: Prediction of Length of Stay on the Intensive Care Unit based on Bayesian Neural Networks\n",
    "1.Dataset: \n",
    "  1)eICU-CRD, https://physionet.org/content/eicu-crd/2.0/   and    https://eicu-crd.mit.edu/ \n",
    "  2)Abstract: The eICU Collaborative Research Database is a multi-center database comprising deidentified health data associated with                over 200,000 admissions to ICUs across the United States between 2014-2015. The database includes vital sign                        measurements, care plan documentation, severity of illness measures, diagnosis information, and treatment information.                Data is collected through the Philips eICU program, a critical care telehealth program that delivers information to                  caregivers at the bedside. \n",
    "  3)Table used: apacheApsVar, apachePredVar, apachePatientResult\n",
    "  \n",
    "2.Task: \n",
    "  1)Apache(Acute Physiology and Chronic Health Evaluation) IV scoring system have been used widely in the intensive care unit(ICU).\n",
    "  2)Critical care medicine Journal-2006: Acute Physiology and Chronic Health Evaluation (APACHE) IV: hospital mortality assessment for today’s critically ill patients.\n",
    "  3)Predict length of stay (los) and Mortality, considering the degree of disease but not directly mortality.\n",
    "  \n",
    "3.Model: \n",
    "  The performance of BNN,DNN,LS+L1,LS+L2,RF on anti-overfitting."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Dataset：trainset, valset, testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of trainset is : 87190,52\n",
      "The shape of valset is : 21798,52\n",
      "The shape of testset is : 27248,52\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#trainset\n",
    "dataset = pd.read_csv(\"/data/fjsdata/physionet/eICU-CRD/EMBC2020/trainset.csv\",sep=',',index_col=['patientunitstayid']) \n",
    "#min-max scale the continous features\n",
    "ss = MinMaxScaler()\n",
    "scale_features = ['ph', 'creatinine', 'albumin','diagnosis']\n",
    "dataset[scale_features] = ss.fit_transform(dataset[scale_features])\n",
    "X_data = dataset.drop(columns=[\"actualiculos\"], inplace=False)  #feature\n",
    "y_data = dataset['actualiculos']#label\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=1)\n",
    "print ('The shape of trainset is : %d,%d'%(X_train.shape[0],X_train.shape[1]))\n",
    "print ('The shape of valset is : %d,%d'%(X_val.shape[0],X_val.shape[1]))\n",
    "\n",
    "#testset\n",
    "testset = pd.read_csv(\"/data/fjsdata/physionet/eICU-CRD/EMBC2020/testset.csv\",sep=',',index_col=['patientunitstayid'])\n",
    "testset[scale_features] = ss.fit_transform(testset[scale_features])\n",
    "X_test = testset.drop(columns=[\"actualiculos\"], inplace=False)  #feature\n",
    "y_test = testset['actualiculos']#label \n",
    "print ('The shape of testset is : %d,%d'%(X_test.shape[0],X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2.LS+L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Score of LS+L1 on eICU-CRD valset is : 1.9846851549588076\n",
      "R^2 Score of LS+L1 on eICU-CRD valset is : 0.11381382199829237\n",
      "EV Score of LS+L1 on eICU-CRD valset is : 0.11382110898999853\n",
      "MAE Score of LS+L1 on eICU-CRD testset is : 2.0118698041687835\n",
      "R^2 Score of LS+L1 on eICU-CRD testset is : 0.09112147903444556\n",
      "EV Score of LS+L1 on eICU-CRD testset is : 0.0911734768295498\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score,mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#1. training model\n",
    "param_grid = {'fit_intercept':[True,False],'alpha':[0.01,0.05,0.1,0.5]}\n",
    "clf = linear_model.Lasso(normalize=False,random_state=0) #max_iter\n",
    "grid_clf = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid_clf.fit(X_data, y_data.ravel())\n",
    "\n",
    "#2. valset performance\n",
    "y_pred_val = grid_clf.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred_val)\n",
    "print(\"MAE Score of LS+L1 on eICU-CRD valset is :\", mae)  \n",
    "r2 = r2_score(y_val, y_pred_val)\n",
    "print(\"R^2 Score of LS+L1 on eICU-CRD valset is :\", r2) \n",
    "ev = explained_variance_score(y_val, y_pred_val)\n",
    "print(\"EV Score of LS+L1 on eICU-CRD valset is :\", ev)\n",
    "\n",
    "#3. testset performance\n",
    "y_pred_test = grid_clf.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred_test)\n",
    "print(\"MAE Score of LS+L1 on eICU-CRD testset is :\", mae)  \n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print(\"R^2 Score of LS+L1 on eICU-CRD testset is :\", r2) \n",
    "ev = explained_variance_score(y_test, y_pred_test)\n",
    "print(\"EV Score of LS+L1 on eICU-CRD testset is :\", ev)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3.LS+L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Score of LS+L2 on eICU-CRD valset is : 1.9786537124978056\n",
      "R^2 Score of LS+L2 on eICU-CRD valset is : 0.11926766341787076\n",
      "EV Score of LS+L2 on eICU-CRD valset is : 0.11926781619540616\n",
      "MAE Score of LS+L2 on eICU-CRD testset is : 2.010006093614097\n",
      "R^2 Score of LS+L2 on eICU-CRD testset is : 0.09309526751210051\n",
      "EV Score of LS+L2 on eICU-CRD testset is : 0.09312159570824141\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score,mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#1. training model\n",
    "param_grid = {'fit_intercept':[True,False],'alpha':[0.01,0.05,0.1,0.5]}\n",
    "clf = linear_model.Ridge(normalize=False,random_state=0) #max_iter\n",
    "grid_clf = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid_clf.fit(X_data, y_data.ravel())\n",
    "\n",
    "#2. valset performance\n",
    "y_pred_val = grid_clf.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred_val)\n",
    "print(\"MAE Score of LS+L2 on eICU-CRD valset is :\", mae)  \n",
    "r2 = r2_score(y_val, y_pred_val)\n",
    "print(\"R^2 Score of LS+L2 on eICU-CRD valset is :\", r2) \n",
    "ev = explained_variance_score(y_val, y_pred_val)\n",
    "print(\"EV Score of LS+L2 on eICU-CRD valset is :\", ev)\n",
    "\n",
    "#3. testset performance\n",
    "y_pred_test = grid_clf.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred_test)\n",
    "print(\"MAE Score of LS+L2 on eICU-CRD testset is :\", mae)  \n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print(\"R^2 Score of LS+L2 on eICU-CRD testset is :\", r2) \n",
    "ev = explained_variance_score(y_test, y_pred_test)\n",
    "print(\"EV Score of LS+L2 on eICU-CRD testset is :\", ev)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4.RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Score of RandomForest on eICU-CRD valset is : 1.8528945201717582\n",
      "R^2 Score of RandomForest on eICU-CRD valset is : 0.21133307430415182\n",
      "EV Score of RandomForest on eICU-CRD valset is : 0.21133361822793006\n",
      "MAE Score ofRandomForest on eICU-CRD testset is : 1.9641036038990178\n",
      "R^2 Score of RandomForest on eICU-CRD testset is : 0.11057098058538672\n",
      "EV Score of RandomForest on eICU-CRD testset is : 0.11064609995353891\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score,mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#1. training model\n",
    "param_grid = { 'n_estimators': [5, 10, 15, 20], 'max_depth': [10, 20, 30, 50] }\n",
    "clf = RandomForestRegressor(max_features='sqrt', min_samples_split=110, min_samples_leaf=20, oob_score=False, random_state=0)\n",
    "grid_clf = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid_clf.fit(X_data, y_data.ravel())\n",
    "\n",
    "#2. valset performance\n",
    "y_pred_val = grid_clf.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred_val)\n",
    "print(\"MAE Score of RandomForest on eICU-CRD valset is :\", mae)  \n",
    "r2 = r2_score(y_val, y_pred_val)\n",
    "print(\"R^2 Score of RandomForest on eICU-CRD valset is :\", r2) \n",
    "ev = explained_variance_score(y_val, y_pred_val)\n",
    "print(\"EV Score of RandomForest on eICU-CRD valset is :\", ev)\n",
    "\n",
    "#3. testset performance\n",
    "y_pred_test = grid_clf.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred_test)\n",
    "print(\"MAE Score ofRandomForest on eICU-CRD testset is :\", mae)  \n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print(\"R^2 Score of RandomForest on eICU-CRD testset is :\", r2) \n",
    "ev = explained_variance_score(y_test, y_pred_test)\n",
    "print(\"EV Score of RandomForest on eICU-CRD testset is :\", ev)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5. DeepNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 / 109 : loss = 86.83037567138672\n",
      "100 / 109 : loss = 55.04607391357422978515625\n",
      "100 / 109 : loss = 41.34906387329101640551758\n",
      "100 / 109 : loss = 33.08215332031256815551758\n",
      "100 / 109 : loss = 27.847930908203125979003906\n",
      "100 / 109 : loss = 24.41364097595215387084961\n",
      "100 / 109 : loss = 22.08343887329101699609375\n",
      "100 / 109 : loss = 20.454273223876953533569336\n",
      "100 / 109 : loss = 19.287708282470703685913086\n",
      "100 / 109 : loss = 18.43716049194336846435547\n",
      "100 / 109 : loss = 17.807607650756836494628906\n",
      "100 / 109 : loss = 17.33424949645996626220703\n",
      "100 / 109 : loss = 16.971525192260742400634766\n",
      "100 / 109 : loss = 16.687166213989258704589844\n",
      "100 / 109 : loss = 16.458526611328125192260742\n",
      "100 / 109 : loss = 16.269981384277344080505371\n",
      "100 / 109 : loss = 16.110879898071298012695312\n",
      "100 / 109 : loss = 15.973997116088867329833984\n",
      "100 / 109 : loss = 15.854377746582031344970703\n",
      "100 / 109 : loss = 15.748559951782227836547852\n",
      "100 / 109 : loss = 15.654035568237305243103027\n",
      "100 / 109 : loss = 15.568944931030273330871582\n",
      "100 / 109 : loss = 15.49185943603515670489502\n",
      "100 / 109 : loss = 15.421673774719238177978516\n",
      "100 / 109 : loss = 15.35751152038574276586914\n",
      "100 / 109 : loss = 15.298688888549805319824219\n",
      "100 / 109 : loss = 15.24465370178222761126709\n",
      "100 / 109 : loss = 15.19496726989746129663086\n",
      "100 / 109 : loss = 15.149274826049805612243652\n",
      "100 / 109 : loss = 15.107278823852539949401855\n",
      "100 / 109 : loss = 15.068725585937588827209473\n",
      "100 / 109 : loss = 15.03339099884033221685791\n",
      "100 / 109 : loss = 15.001068115234375838745117\n",
      "100 / 109 : loss = 14.971565246582031652893066\n",
      "100 / 109 : loss = 14.94470024108886761932373\n",
      "100 / 109 : loss = 14.92029762268066426196289\n",
      "100 / 109 : loss = 14.898187637329102965759277\n",
      "100 / 109 : loss = 14.878209114074707745056152\n",
      "100 / 109 : loss = 14.860219955444336919311523\n",
      "100 / 109 : loss = 14.844078063964844900634766\n",
      "100 / 109 : loss = 14.829646110534668498901367\n",
      "100 / 109 : loss = 14.816800117492676292358398\n",
      "100 / 109 : loss = 14.805412292480469534973145\n",
      "100 / 109 : loss = 14.795361518859863619689941\n",
      "100 / 109 : loss = 14.786532402038574012939453\n",
      "100 / 109 : loss = 14.778811454772950061340332\n",
      "100 / 109 : loss = 14.772094726562525721740723\n",
      "100 / 109 : loss = 14.766276359558105414245605\n",
      "100 / 109 : loss = 14.761265754699707586120605\n",
      "100 / 109 : loss = 14.756975173950195545654297\n",
      "100 / 109 : loss = 14.753323554992676462158203\n",
      "100 / 109 : loss = 14.750242233276367485778809\n",
      "100 / 109 : loss = 14.747667312622075812988281\n",
      "Mean loss in this epoch is: 14.053152084350586MAE Score of DeepNN on eICU-CRD valset is : 1.9891808768629142\n",
      "R^2 Score of DeepNN on eICU-CRD valset is : 0.11841541965869529\n",
      "EV Score of DeepNN on eICU-CRD valset is : 0.11844146977505654\n",
      "MAE Score of DeepNN on eICU-CRD testset is : 2.01927910446287\n",
      "R^2 Score of DeepNN on eICU-CRD testset is : 0.09255443179476053\n",
      "EV Score of DeepNN on eICU-CRD testset is : 0.09255446450562377\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score,mean_squared_error\n",
    "from math import sqrt\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#DNN model\n",
    "class TF_DNNRegressor_eICU:\n",
    "    def __init__(self, lr=0.001, dim=52, num_class=1, batchSize=1000):\n",
    "        #global parameters\n",
    "        self.lr = lr\n",
    "        self.dim = dim # dimensions of sample\n",
    "        self.num_class = num_class #output \n",
    "        self.hidden_layers = [16, 4]\n",
    "        #set network structure\n",
    "        self.add_placeholders()\n",
    "        self.add_weight()\n",
    "        self.add_model()\n",
    "        self.add_loss()\n",
    "        self.add_optimizer()\n",
    "        self.init_sess()\n",
    "        \n",
    "    def add_placeholders(self):    \n",
    "        self.X_input = tf.placeholder(\"float\", [None, self.dim])\n",
    "        self.Y_input = tf.placeholder(\"float\", [None, self.num_class])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)  \n",
    "    \n",
    "    def add_weight(self):\n",
    "        # Store layers weight & bias\n",
    "        #init_uniform = tf.random_uniform_initializer(minval=0, maxval=1, seed=None, dtype=tf.float32)\n",
    "        self.weights = {\n",
    "            'w1': tf.Variable(tf.random_normal([self.dim, self.hidden_layers[0]])),\n",
    "            'w2': tf.Variable(tf.random_normal([self.hidden_layers[0], self.hidden_layers[1]])),\n",
    "            'wout': tf.Variable(tf.random_normal([self.hidden_layers[1], self.num_class]))\n",
    "        }\n",
    "        self.biases = {\n",
    "            'b1': tf.Variable(tf.random_normal([self.hidden_layers[0]])),\n",
    "            'b2': tf.Variable(tf.random_normal([self.hidden_layers[1]])),\n",
    "            'bout': tf.Variable(tf.random_normal([self.num_class]))\n",
    "        }\n",
    "        \n",
    "    def add_model(self):\n",
    "        # Hidden fully connected layer with 16 neurons\n",
    "        layer_1 =  tf.add(tf.matmul(self.X_input, self.weights['w1']), self.biases['b1']) \n",
    "        # Hidden fully connected layer with 4 neurons\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, self.weights['w2']), self.biases['b2']) \n",
    "        # Output fully connected layer with a neuron for each class\n",
    "        out_layer =tf.matmul(layer_2, self.weights['wout']) + self.biases['bout'] \n",
    "        self.Y_output =  out_layer\n",
    "    \n",
    "    def add_loss(self):\n",
    "         self.loss = tf.losses.mean_squared_error( self.Y_input , self.Y_output ) \n",
    "            \n",
    "    def add_optimizer(self):\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_step = optimizer.minimize(self.loss)\n",
    "        \n",
    "    def init_sess(self):\n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        self.config.allow_soft_placement = True\n",
    "        #self.config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "        self.sess = tf.Session(config=self.config)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "#1.training model\n",
    "tf_model = TF_DNNRegressor_eICU()\n",
    "#set paramete\n",
    "verbose = 10\n",
    "batchSize=1000\n",
    "num_batches = X_data.shape[0] // batchSize + 1 \n",
    "pre_loss = 0.0\n",
    "while True:#convergence\n",
    "    losses = []\n",
    "    for i in range(num_batches):\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([X_data.shape[0], (i+1)*batchSize])\n",
    "        X_batch = X_data[min_idx: max_idx]\n",
    "        Y_batch = y_data.to_frame()[min_idx: max_idx]\n",
    "        _, tmp_loss,  = tf_model.sess.run([tf_model.train_step, tf_model.loss], \n",
    "                                         feed_dict={tf_model.X_input: X_batch,tf_model.Y_input: Y_batch, tf_model.keep_prob: 0.6})\n",
    "        losses.append(tmp_loss)\n",
    "        if verbose and i % verbose == 0:\n",
    "            sys.stdout.write('\\r{} / {} : loss = {}'.format(i, num_batches, np.mean(losses[-verbose:])))\n",
    "            sys.stdout.flush()\n",
    "    sys.stdout.write(\"\\nMean loss in this epoch is: {}\".format( np.mean(losses) ))\n",
    "    sys.stdout.flush()\n",
    "    #whether convergence\n",
    "    if abs( np.mean(losses) - pre_loss)<0.001:\n",
    "        break\n",
    "    else:\n",
    "        pre_loss = np.mean(losses)\n",
    "        \n",
    "#2. valset performance\n",
    "y_pred_val = tf_model.sess.run(tf_model.Y_output, feed_dict={tf_model.X_input: X_val,tf_model.Y_input: y_val.to_frame(), tf_model.keep_prob: 1})\n",
    "mae = mean_absolute_error(y_val, y_pred_val)\n",
    "print(\"MAE Score of DeepNN on eICU-CRD valset is :\", mae)  \n",
    "r2 = r2_score(y_val, y_pred_val)\n",
    "print(\"R^2 Score of DeepNN on eICU-CRD valset is :\", r2) \n",
    "ev = explained_variance_score(y_val, y_pred_val)\n",
    "print(\"EV Score of DeepNN on eICU-CRD valset is :\", ev)\n",
    "\n",
    "#3. testset performance\n",
    "y_pred_test = tf_model.sess.run(tf_model.Y_output, feed_dict={tf_model.X_input: X_test,tf_model.Y_input: y_test.to_frame(), tf_model.keep_prob: 1})\n",
    "mae = mean_absolute_error(y_test, y_pred_test)\n",
    "print(\"MAE Score of DeepNN on eICU-CRD testset is :\", mae)  \n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print(\"R^2 Score of DeepNN on eICU-CRD testset is :\", r2) \n",
    "ev = explained_variance_score(y_test, y_pred_test)\n",
    "print(\"EV Score of DeepNN on eICU-CRD testset is :\", ev)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6.BayesianNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "9.0.176\n",
      "GeForce RTX 2080 Ti\n",
      "Epoch: 000/300, loss = 4.199\n",
      "Epoch: 010/300, loss = 4.082\n",
      "Epoch: 020/300, loss = 3.977\n",
      "Epoch: 030/300, loss = 3.873\n",
      "Epoch: 040/300, loss = 3.772\n",
      "Epoch: 050/300, loss = 3.672\n",
      "Epoch: 060/300, loss = 3.575\n",
      "Epoch: 070/300, loss = 3.482\n",
      "Epoch: 080/300, loss = 3.393\n",
      "Epoch: 090/300, loss = 3.309\n",
      "Epoch: 100/300, loss = 3.230\n",
      "Epoch: 110/300, loss = 3.157\n",
      "Epoch: 120/300, loss = 3.092\n",
      "Epoch: 130/300, loss = 3.034\n",
      "Epoch: 140/300, loss = 2.985\n",
      "Epoch: 150/300, loss = 2.943\n",
      "Epoch: 160/300, loss = 2.911\n",
      "Epoch: 170/300, loss = 2.884\n",
      "Epoch: 180/300, loss = 2.864\n",
      "Epoch: 190/300, loss = 2.850\n",
      "Epoch: 200/300, loss = 2.838\n",
      "Epoch: 210/300, loss = 2.829\n",
      "Epoch: 220/300, loss = 2.820\n",
      "Epoch: 230/300, loss = 2.815\n",
      "Epoch: 240/300, loss = 2.808\n",
      "Epoch: 250/300, loss = 2.804\n",
      "Epoch: 260/300, loss = 2.799\n",
      "Epoch: 270/300, loss = 2.796\n",
      "Epoch: 280/300, loss = 2.791\n",
      "Epoch: 290/300, loss = 2.790\n",
      "Epoch: 299/300, loss = 2.785\n",
      "The best loss = 2.785\n",
      "MAE Score of BNN on eICU-CRD valset is : 1.932150397428205\n",
      "R^2 Score of BNN on eICU-CRD valset is : 0.12846324995276526\n",
      "EV Score of BNN on eICU-CRD valset is : 0.12903888140651965\n",
      "MAE Score of BNN on eICU-CRD testset is : 1.9550446939099733\n",
      "R^2 Score of BNN on eICU-CRD testset is : 0.0979095129748232\n",
      "EV Score of BNN on eICU-CRD testset is : 0.09897135699104742\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import datasets, transforms\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score,mean_squared_error\n",
    "print (torch.cuda.is_available())\n",
    "print (torch.version.cuda)\n",
    "print (torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "#model\n",
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        \n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "def log_gaussian_loss(output, target, sigma, no_dim, sum_reduce=True):\n",
    "    exponent = -0.5*(target - output)**2/sigma**2\n",
    "    log_coeff = -no_dim*torch.log(sigma) - 0.5*no_dim*np.log(2*np.pi)\n",
    "    \n",
    "    if sum_reduce:\n",
    "        return -(log_coeff + exponent).sum()\n",
    "    else:\n",
    "        return -(log_coeff + exponent)\n",
    "    \n",
    "class gaussian:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def loglik(self, weights):\n",
    "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
    "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
    "        \n",
    "        return (exponent + log_coeff).sum()\n",
    "    \n",
    "class BayesLinear_Normalq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, prior):\n",
    "        super(BayesLinear_Normalq, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.weight_mus = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
    "        self.weight_rhos = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-3, -3))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # sample gaussian noise for each weight\n",
    "        weight_epsilons = Variable(self.weight_mus.data.new(self.weight_mus.size()).normal_())      \n",
    "        # calculate the weight stds from the rho parameters\n",
    "        weight_stds = torch.log(1 + torch.exp(self.weight_rhos))\n",
    "        # calculate samples from the posterior from the sampled noise and mus/stds\n",
    "        weight_sample = self.weight_mus + weight_epsilons*weight_stds\n",
    "            \n",
    "        torch.cuda.synchronize()\n",
    "        output = torch.mm(x, weight_sample)\n",
    "            \n",
    "        # computing the KL loss term\n",
    "        #reference: https://github.com/jojonki/AutoEncoders/blob/master/kl_divergence_between_two_gaussians.pdf\n",
    "        prior_cov, varpost_cov = self.prior.sigma**2, weight_stds**2\n",
    "        KL_loss = 0.5*(torch.log(prior_cov/varpost_cov)).sum() - 0.5*weight_stds.numel()\n",
    "        KL_loss = KL_loss + 0.5*(varpost_cov/prior_cov).sum()\n",
    "        KL_loss = KL_loss + 0.5*((self.weight_mus - self.prior.mu)**2/prior_cov).sum()\n",
    "            \n",
    "        return output, KL_loss\n",
    "    \n",
    "class BBP_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_units):\n",
    "        super(BBP_Model, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # network with two hidden and one output layer\n",
    "        self.layer1 = BayesLinear_Normalq(input_dim, num_units[0], gaussian(0, 3))\n",
    "        self.layer2 = BayesLinear_Normalq(num_units[0], num_units[1], gaussian(0, 3))\n",
    "        self.layer3 = BayesLinear_Normalq(num_units[1], output_dim, gaussian(0, 3))\n",
    "        \n",
    "        # activation to be used between hidden layers\n",
    "        self.activation = nn.ReLU(inplace = True)\n",
    "        # noise\n",
    "        self.log_noise = nn.Parameter(torch.cuda.FloatTensor([3]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        KL_loss_total = 0\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        \n",
    "        x, KL_loss = self.layer1(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x, KL_loss = self.layer2(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x, KL_loss = self.layer3(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return x, KL_loss_total\n",
    "    \n",
    "class BBP_Model_Wrapper:\n",
    "    def __init__(self, network, learn_rate=1e-2):\n",
    "        \n",
    "        self.learn_rate = learn_rate\n",
    "        self.network = network\n",
    "        self.network.cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = self.learn_rate)\n",
    "        self.loss_func = log_gaussian_loss#nn.MSELoss() \n",
    "    \n",
    "    def fit(self, x, y, no_samples):\n",
    "        \n",
    "        x, y = to_variable(var=(x, y), cuda=True)\n",
    "        \n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "        fit_loss_total = 0\n",
    "        KL_loss_total = 0\n",
    "        for i in range(no_samples):\n",
    "            output, KL_loss = self.network(x)\n",
    "            KL_loss_total = KL_loss_total + KL_loss\n",
    "            # calculate fit loss based on mean and standard deviation of output\n",
    "            fit_loss = self.loss_func(output, y, self.network.log_noise.exp(), 1) \n",
    "            fit_loss_total = fit_loss_total + fit_loss\n",
    "        \n",
    "        total_loss = (fit_loss_total + KL_loss_total)/(no_samples*x.shape[0])\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "#1.training model\n",
    "num_epochs = 300\n",
    "log_every=10\n",
    "best_net, best_loss = None, float('inf')\n",
    "net = BBP_Model_Wrapper(network=BBP_Model(input_dim=X_data.shape[1], output_dim=1, num_units=[128,32]))\n",
    "for i in range(num_epochs):\n",
    "    total_loss = net.fit(np.array(X_data), np.array(y_data.to_frame()), no_samples = 100)\n",
    "\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        best_net = copy.deepcopy(net.network)\n",
    "            \n",
    "    if i % log_every == 0 or i == num_epochs - 1:\n",
    "        print('Epoch: %s/%d, loss = %.3f' %(str(i).zfill(3), num_epochs, total_loss))\n",
    "        \n",
    "print ('The best loss = %.3f'%(best_loss))\n",
    "        \n",
    "#2. valset performance\n",
    "X_val_cuda, y_val_cuda = to_variable(var=(np.array(X_val), np.array(y_val.to_frame())), cuda=True)\n",
    "y_pred_val= []\n",
    "for i in range(200):#sample\n",
    "    output, KL_loss = best_net(X_val_cuda)\n",
    "    y_pred_val.append(output.cpu().data.numpy())\n",
    "y_pred_val = np.array(y_pred_val).mean(axis=0)    \n",
    "mae=mean_absolute_error(y_val, y_pred_val)\n",
    "r2=r2_score(y_val, y_pred_val)\n",
    "ev=explained_variance_score(y_val, y_pred_val)\n",
    "print(\"MAE Score of BNN on eICU-CRD valset is :\", mae)    \n",
    "print(\"R^2 Score of BNN on eICU-CRD valset is :\", r2)  \n",
    "print(\"EV Score of BNN on eICU-CRD valset is :\", ev) \n",
    "\n",
    "#3. testset performance\n",
    "X_test_cuda, y_test_cuda = to_variable(var=(np.array(X_test), np.array(y_test.to_frame())), cuda=True)\n",
    "y_test_val= []\n",
    "for i in range(200):#sample\n",
    "    output, KL_loss = best_net(X_test_cuda)\n",
    "    y_test_val.append(output.cpu().data.numpy())\n",
    "y_test_val = np.array(y_test_val).mean(axis=0)  \n",
    "mae=mean_absolute_error(y_test, y_test_val)\n",
    "r2=r2_score(y_test, y_test_val)\n",
    "ev=explained_variance_score(y_test, y_test_val)\n",
    "print(\"MAE Score of BNN on eICU-CRD testset is :\", mae)    \n",
    "print(\"R^2 Score of BNN on eICU-CRD testset is :\", r2)  \n",
    "print(\"EV Score of BNN on eICU-CRD testset is :\", ev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "9.0.176\n",
      "GeForce RTX 2080 Ti\n",
      "Epoch: 000/500, loss = 4.308\n",
      "Epoch: 010/500, loss = 4.191\n",
      "Epoch: 020/500, loss = 4.087\n",
      "Epoch: 030/500, loss = 3.982\n",
      "Epoch: 040/500, loss = 3.880\n",
      "Epoch: 050/500, loss = 3.781\n",
      "Epoch: 060/500, loss = 3.684\n",
      "Epoch: 070/500, loss = 3.590\n",
      "Epoch: 080/500, loss = 3.501\n",
      "Epoch: 090/500, loss = 3.416\n",
      "Epoch: 100/500, loss = 3.338\n",
      "Epoch: 110/500, loss = 3.264\n",
      "Epoch: 120/500, loss = 3.199\n",
      "Epoch: 130/500, loss = 3.140\n",
      "Epoch: 140/500, loss = 3.091\n",
      "Epoch: 150/500, loss = 3.050\n",
      "Epoch: 160/500, loss = 3.014\n",
      "Epoch: 170/500, loss = 2.988\n",
      "Epoch: 180/500, loss = 2.967\n",
      "Epoch: 190/500, loss = 2.951\n",
      "Epoch: 200/500, loss = 2.939\n",
      "Epoch: 210/500, loss = 2.928\n",
      "Epoch: 220/500, loss = 2.923\n",
      "Epoch: 230/500, loss = 2.913\n",
      "Epoch: 240/500, loss = 2.907\n",
      "Epoch: 250/500, loss = 2.901\n",
      "Epoch: 260/500, loss = 2.894\n",
      "Epoch: 270/500, loss = 2.889\n",
      "Epoch: 280/500, loss = 2.882\n",
      "Epoch: 290/500, loss = 2.879\n",
      "Epoch: 300/500, loss = 2.873\n",
      "Epoch: 310/500, loss = 2.869\n",
      "Epoch: 320/500, loss = 2.867\n",
      "Epoch: 330/500, loss = 2.861\n",
      "Epoch: 340/500, loss = 2.856\n",
      "Epoch: 350/500, loss = 2.852\n",
      "Epoch: 360/500, loss = 2.848\n",
      "Epoch: 370/500, loss = 2.844\n",
      "Epoch: 380/500, loss = 2.840\n",
      "Epoch: 390/500, loss = 2.837\n",
      "Epoch: 400/500, loss = 2.832\n",
      "Epoch: 410/500, loss = 2.829\n",
      "Epoch: 420/500, loss = 2.826\n",
      "Epoch: 430/500, loss = 2.827\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import datasets, transforms\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score,mean_squared_error\n",
    "print (torch.cuda.is_available())\n",
    "print (torch.version.cuda)\n",
    "print (torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "#model\n",
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        \n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "def log_gaussian_loss(output, target, sigma, no_dim, sum_reduce=True):\n",
    "    exponent = -0.5*(target - output)**2/sigma**2\n",
    "    log_coeff = -no_dim*torch.log(sigma) - 0.5*no_dim*np.log(2*np.pi)\n",
    "    \n",
    "    if sum_reduce:\n",
    "        return -(log_coeff + exponent).sum()\n",
    "    else:\n",
    "        return -(log_coeff + exponent)\n",
    "    \n",
    "class gaussian:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def loglik(self, weights):\n",
    "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
    "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
    "        \n",
    "        return (exponent + log_coeff).sum()\n",
    "    \n",
    "class BayesLinear_Normalq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, prior):\n",
    "        super(BayesLinear_Normalq, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.weight_mus = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
    "        self.weight_rhos = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-3, -3))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # sample gaussian noise for each weight\n",
    "        weight_epsilons = Variable(self.weight_mus.data.new(self.weight_mus.size()).normal_())      \n",
    "        # calculate the weight stds from the rho parameters\n",
    "        weight_stds = torch.log(1 + torch.exp(self.weight_rhos))\n",
    "        # calculate samples from the posterior from the sampled noise and mus/stds\n",
    "        weight_sample = self.weight_mus + weight_epsilons*weight_stds\n",
    "            \n",
    "        torch.cuda.synchronize()\n",
    "        output = torch.mm(x, weight_sample)\n",
    "            \n",
    "        # computing the KL loss term\n",
    "        #reference: https://github.com/jojonki/AutoEncoders/blob/master/kl_divergence_between_two_gaussians.pdf\n",
    "        prior_cov, varpost_cov = self.prior.sigma**2, weight_stds**2\n",
    "        KL_loss = 0.5*(torch.log(prior_cov/varpost_cov)).sum() - 0.5*weight_stds.numel()\n",
    "        KL_loss = KL_loss + 0.5*(varpost_cov/prior_cov).sum()\n",
    "        KL_loss = KL_loss + 0.5*((self.weight_mus - self.prior.mu)**2/prior_cov).sum()\n",
    "            \n",
    "        return output, KL_loss\n",
    "    \n",
    "class BBP_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_units):\n",
    "        super(BBP_Model, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # network with two hidden and one output layer\n",
    "        self.layer1 = BayesLinear_Normalq(input_dim, num_units[0], gaussian(0, 3))\n",
    "        self.layer2 = BayesLinear_Normalq(num_units[0], num_units[1], gaussian(0, 3))\n",
    "        self.layer3 = BayesLinear_Normalq(num_units[1], output_dim, gaussian(0, 3))\n",
    "        \n",
    "        # activation to be used between hidden layers\n",
    "        self.activation = nn.ReLU(inplace = True)\n",
    "        # noise\n",
    "        self.log_noise = nn.Parameter(torch.cuda.FloatTensor([3]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        KL_loss_total = 0\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        \n",
    "        x, KL_loss = self.layer1(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x, KL_loss = self.layer2(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x, KL_loss = self.layer3(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return x, KL_loss_total\n",
    "    \n",
    "class BBP_Model_Wrapper:\n",
    "    def __init__(self, network, learn_rate=1e-2):\n",
    "        \n",
    "        self.learn_rate = learn_rate\n",
    "        self.network = network\n",
    "        self.network.cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = self.learn_rate)\n",
    "        self.loss_func = log_gaussian_loss#nn.MSELoss() \n",
    "    \n",
    "    def fit(self, x, y, no_samples):\n",
    "        \n",
    "        x, y = to_variable(var=(x, y), cuda=True)\n",
    "        \n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "        fit_loss_total = 0\n",
    "        KL_loss_total = 0\n",
    "        for i in range(no_samples):\n",
    "            output, KL_loss = self.network(x)\n",
    "            KL_loss_total = KL_loss_total + KL_loss\n",
    "            # calculate fit loss based on mean and standard deviation of output\n",
    "            fit_loss = self.loss_func(output, y, self.network.log_noise.exp(), 1) \n",
    "            fit_loss_total = fit_loss_total + fit_loss\n",
    "        \n",
    "        total_loss = (fit_loss_total + KL_loss_total)/(no_samples*x.shape[0])\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "#1.training model\n",
    "num_epochs = 500\n",
    "log_every=10\n",
    "best_net, best_loss = None, float('inf')\n",
    "net = BBP_Model_Wrapper(network=BBP_Model(input_dim=X_data.shape[1], output_dim=1, num_units=[128,32]))\n",
    "for i in range(num_epochs):\n",
    "    total_loss = net.fit(np.array(X_data), np.array(y_data.to_frame()), no_samples = 100)\n",
    "\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        best_net = copy.deepcopy(net.network)\n",
    "            \n",
    "    if i % log_every == 0 or i == num_epochs - 1:\n",
    "        print('Epoch: %s/%d, loss = %.3f' %(str(i).zfill(3), num_epochs, total_loss))\n",
    "        \n",
    "print ('The best loss = %.3f'%(best_loss))\n",
    "        \n",
    "#2. valset performance\n",
    "X_val_cuda, y_val_cuda = to_variable(var=(np.array(X_val), np.array(y_val.to_frame())), cuda=True)\n",
    "y_pred_val= []\n",
    "for i in range(200):#sample\n",
    "    output, KL_loss = best_net(X_val_cuda)\n",
    "    y_pred_val.append(output.cpu().data.numpy())\n",
    "y_pred_val = np.array(y_pred_val).mean(axis=0)    \n",
    "mae=mean_absolute_error(y_val, y_pred_val)\n",
    "r2=r2_score(y_val, y_pred_val)\n",
    "ev=explained_variance_score(y_val, y_pred_val)\n",
    "print(\"MAE Score of BNN on eICU-CRD valset is :\", mae)    \n",
    "print(\"R^2 Score of BNN on eICU-CRD valset is :\", r2)  \n",
    "print(\"EV Score of BNN on eICU-CRD valset is :\", ev) \n",
    "\n",
    "#3. testset performance\n",
    "X_test_cuda, y_test_cuda = to_variable(var=(np.array(X_test), np.array(y_test.to_frame())), cuda=True)\n",
    "y_test_val= []\n",
    "for i in range(200):#sample\n",
    "    output, KL_loss = best_net(X_test_cuda)\n",
    "    y_test_val.append(output.cpu().data.numpy())\n",
    "y_test_val = np.array(y_test_val).mean(axis=0)  \n",
    "mae=mean_absolute_error(y_test, y_test_val)\n",
    "r2=r2_score(y_test, y_test_val)\n",
    "ev=explained_variance_score(y_test, y_test_val)\n",
    "print(\"MAE Score of BNN on eICU-CRD testset is :\", mae)    \n",
    "print(\"R^2 Score of BNN on eICU-CRD testset is :\", r2)  \n",
    "print(\"EV Score of BNN on eICU-CRD testset is :\", ev) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch parallel training,invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "9.0.176\n",
      "GeForce RTX 2080 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/200, loss = 4.639\n",
      "Epoch: 010/200, loss = 4.520\n",
      "Epoch: 020/200, loss = 4.394\n",
      "Epoch: 030/200, loss = 4.257\n",
      "Epoch: 040/200, loss = 4.117\n",
      "Epoch: 050/200, loss = 3.975\n",
      "Epoch: 060/200, loss = 3.837\n",
      "Epoch: 070/200, loss = 3.707\n",
      "Epoch: 080/200, loss = 3.589\n",
      "Epoch: 090/200, loss = 3.490\n",
      "Epoch: 100/200, loss = 3.416\n",
      "Epoch: 110/200, loss = 3.376\n",
      "Epoch: 120/200, loss = 3.377\n",
      "Epoch: 130/200, loss = 3.427\n",
      "Epoch: 140/200, loss = 3.525\n",
      "Epoch: 150/200, loss = 3.657\n",
      "Epoch: 160/200, loss = 3.787\n",
      "Epoch: 170/200, loss = 3.856\n",
      "Epoch: 180/200, loss = 3.810\n",
      "Epoch: 190/200, loss = 3.646\n",
      "Epoch: 199/200, loss = 3.450\n",
      "The best loss = 3.371\n",
      "MAE Score of BNN on eICU-CRD valset is : 2.929296156678186\n",
      "R^2 Score of BNN on eICU-CRD valset is : -0.5817041778406427\n",
      "EV Score of BNN on eICU-CRD valset is : -4.513833751218499e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Score of BNN on eICU-CRD testset is : 2.9506505944370716\n",
      "R^2 Score of BNN on eICU-CRD testset is : -0.4412636601685773\n",
      "EV Score of BNN on eICU-CRD testset is : 3.4406244076023995e-08\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import datasets, transforms\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score,mean_squared_error\n",
    "print (torch.cuda.is_available())\n",
    "print (torch.version.cuda)\n",
    "print (torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "#model\n",
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        \n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "def log_gaussian_loss(output, target, sigma, no_dim, sum_reduce=True):\n",
    "    exponent = -0.5*(target - output)**2/sigma**2\n",
    "    log_coeff = -no_dim*torch.log(sigma) - 0.5*no_dim*np.log(2*np.pi)\n",
    "    \n",
    "    if sum_reduce:\n",
    "        return -(log_coeff + exponent).sum()\n",
    "    else:\n",
    "        return -(log_coeff + exponent)\n",
    "    \n",
    "class gaussian:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def loglik(self, weights):\n",
    "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
    "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
    "        \n",
    "        return (exponent + log_coeff).sum()\n",
    "    \n",
    "class BayesLinear_Normalq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, prior):\n",
    "        super(BayesLinear_Normalq, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.weight_mus = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
    "        self.weight_rhos = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-3, -3))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # sample gaussian noise for each weight\n",
    "        weight_epsilons = Variable(self.weight_mus.data.new(self.weight_mus.size()).normal_())      \n",
    "        # calculate the weight stds from the rho parameters\n",
    "        weight_stds = torch.log(1 + torch.exp(self.weight_rhos))\n",
    "        # calculate samples from the posterior from the sampled noise and mus/stds\n",
    "        weight_sample = self.weight_mus + weight_epsilons*weight_stds\n",
    "            \n",
    "        #torch.cuda.synchronize()\n",
    "        output = torch.mm(x, weight_sample)\n",
    "            \n",
    "        # computing the KL loss term\n",
    "        #reference: https://github.com/jojonki/AutoEncoders/blob/master/kl_divergence_between_two_gaussians.pdf\n",
    "        prior_cov, varpost_cov = self.prior.sigma**2, weight_stds**2\n",
    "        KL_loss = 0.5*(torch.log(prior_cov/varpost_cov)).sum() - 0.5*weight_stds.numel()\n",
    "        KL_loss = KL_loss + 0.5*(varpost_cov/prior_cov).sum()\n",
    "        KL_loss = KL_loss + 0.5*((self.weight_mus - self.prior.mu)**2/prior_cov).sum()\n",
    "            \n",
    "        return output, KL_loss\n",
    "    \n",
    "class BBP_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_units):\n",
    "        super(BBP_Model, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        # network with two hidden and one output layer\n",
    "        self.layer1 = BayesLinear_Normalq(input_dim, num_units[0], gaussian(0, 1))\n",
    "        self.layer2 = BayesLinear_Normalq(num_units[0], num_units[1], gaussian(0, 1))\n",
    "        self.layer3 = BayesLinear_Normalq(num_units[1], output_dim, gaussian(0, 1))\n",
    "        \n",
    "        # activation to be used between hidden layers\n",
    "        self.activation = nn.ReLU(inplace = True)\n",
    "        # noise\n",
    "        self.log_noise = nn.Parameter(torch.cuda.FloatTensor([3]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        KL_loss_total = 0\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        \n",
    "        x, KL_loss = self.layer1(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x, KL_loss = self.layer2(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x, KL_loss = self.layer3(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return x, KL_loss_total\n",
    "    \n",
    "class BBP_Model_Wrapper:\n",
    "    def __init__(self, network, learn_rate=1e-2):\n",
    "        \n",
    "        self.learn_rate = learn_rate\n",
    "        self.loss_func = log_gaussian_loss#nn.MSELoss() \n",
    "        \n",
    "        self.network = network\n",
    "        self.network.cuda()\n",
    "        self.network = nn.DataParallel(self.network,device_ids=[0,1,2,3,4,5,6,7])\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = self.learn_rate)\n",
    "        self.optimizer = nn.DataParallel(self.optimizer,device_ids=[0,1,2,3,4,5,6,7])\n",
    "    \n",
    "    def fit(self, x, y, no_samples):\n",
    "        \n",
    "        x, y = to_variable(var=(x, y), cuda=True)\n",
    "        \n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "        fit_loss_total = 0\n",
    "        KL_loss_total = 0\n",
    "        for i in range(no_samples):\n",
    "            output, KL_loss = self.network(x)\n",
    "            KL_loss_total = KL_loss_total + KL_loss\n",
    "            # calculate fit loss based on mean and standard deviation of output\n",
    "            fit_loss = self.loss_func(output, y, self.network.module.log_noise.exp(), 1) \n",
    "            fit_loss_total = fit_loss_total + fit_loss\n",
    "        \n",
    "        total_loss = (fit_loss_total + KL_loss_total)/(no_samples*x.shape[0])\n",
    "        total_loss.mean().backward()#mean()\n",
    "        self.optimizer.module.step()\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "#1.training model\n",
    "num_epochs = 200\n",
    "log_every=10\n",
    "best_net, best_loss = None, float('inf')\n",
    "net = BBP_Model_Wrapper(network=BBP_Model(input_dim=X_data.shape[1], output_dim=1, num_units=[256,64])) #128,32\n",
    "for i in range(num_epochs):\n",
    "    total_loss = net.fit(np.array(X_data), np.array(y_data.to_frame()), no_samples = 100).mean()\n",
    "\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        best_net = copy.deepcopy(net.network)\n",
    "            \n",
    "    if i % log_every == 0 or i == num_epochs - 1:\n",
    "        print('Epoch: %s/%d, loss = %.3f' %(str(i).zfill(3), num_epochs, total_loss))\n",
    "        \n",
    "print ('The best loss = %.3f'%(best_loss))\n",
    "        \n",
    "#2. valset performance\n",
    "X_val_cuda, y_val_cuda = to_variable(var=(np.array(X_val), np.array(y_val.to_frame())), cuda=True)\n",
    "y_pred_val= []\n",
    "for i in range(200):#sample\n",
    "    output, KL_loss = best_net(X_val_cuda)\n",
    "    y_pred_val.append(output.cpu().data.numpy())\n",
    "y_pred_val = np.array(y_pred_val).mean(axis=0)    \n",
    "mae=mean_absolute_error(y_val, y_pred_val)\n",
    "r2=r2_score(y_val, y_pred_val)\n",
    "ev=explained_variance_score(y_val, y_pred_val)\n",
    "print(\"MAE Score of BNN on eICU-CRD valset is :\", mae)    \n",
    "print(\"R^2 Score of BNN on eICU-CRD valset is :\", r2)  \n",
    "print(\"EV Score of BNN on eICU-CRD valset is :\", ev) \n",
    "\n",
    "#3. testset performance\n",
    "X_test_cuda, y_test_cuda = to_variable(var=(np.array(X_test), np.array(y_test.to_frame())), cuda=True)\n",
    "y_test_val= []\n",
    "for i in range(200):#sample\n",
    "    output, KL_loss = best_net(X_test_cuda)\n",
    "    y_test_val.append(output.cpu().data.numpy())\n",
    "y_test_val = np.array(y_test_val).mean(axis=0)  \n",
    "mae=mean_absolute_error(y_test, y_test_val)\n",
    "r2=r2_score(y_test, y_test_val)\n",
    "ev=explained_variance_score(y_test, y_test_val)\n",
    "print(\"MAE Score of BNN on eICU-CRD testset is :\", mae)    \n",
    "print(\"R^2 Score of BNN on eICU-CRD testset is :\", r2)  \n",
    "print(\"EV Score of BNN on eICU-CRD testset is :\", ev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
