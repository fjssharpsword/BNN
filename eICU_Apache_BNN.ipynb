{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EMBC2020: Prediction of Length of Stay on the Intensive Care Unit based on Bayesian Neural Networks\n",
    "1.Dataset: \n",
    "  1)eICU-CRD, https://physionet.org/content/eicu-crd/2.0/   and    https://eicu-crd.mit.edu/ \n",
    "  2)Abstract: The eICU Collaborative Research Database is a multi-center database comprising deidentified health data associated with                over 200,000 admissions to ICUs across the United States between 2014-2015. The database includes vital sign                        measurements, care plan documentation, severity of illness measures, diagnosis information, and treatment information.                Data is collected through the Philips eICU program, a critical care telehealth program that delivers information to                  caregivers at the bedside. \n",
    "  3)Table used: apacheApsVar, apachePredVar, apachePatientResult\n",
    "  \n",
    "2.Task: \n",
    "  1)Apache(Acute Physiology and Chronic Health Evaluation) IV scoring system have been used widely in the intensive care unit(ICU).\n",
    "  2)Critical care medicine Journal-2006: Acute Physiology and Chronic Health Evaluation (APACHE) IV: hospital mortality assessment for todayâ€™s critically ill patients.\n",
    "  3)Predict length of stay (los) and Mortality, considering the degree of disease but not directly mortality.\n",
    "  \n",
    "3.Model: \n",
    "  1)Baseline-DNN(Deep Neural Networks)\n",
    "  2)BNN (Bayesian Neural Networks) for getting sparse solution to alleviate over-fitting and enhance interpretability.\n",
    "   Sparse Solution->reduce model complexity->enhance interpretability and anti-overfitting \n",
    "   Sparse Learning->denoise.\n",
    "   L1 <=> Laplace Distribution, L2<=>Gaussian Distribution."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2.2 Bayesian Neural Networks with Pytorch\n",
    "https://github.com/JavierAntoran/Bayesian-Neural-Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "9.0.176\n",
      "GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import GPy\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score,mean_squared_error\n",
    "print (torch.cuda.is_available())\n",
    "print (torch.version.cuda)\n",
    "print (torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        \n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "\n",
    "        out.append(v)\n",
    "    return out\n",
    "def log_gaussian_loss(output, target, sigma, no_dim, sum_reduce=True):\n",
    "    exponent = -0.5*(target - output)**2/sigma**2\n",
    "    log_coeff = -no_dim*torch.log(sigma) - 0.5*no_dim*np.log(2*np.pi)\n",
    "    \n",
    "    if sum_reduce:\n",
    "        return -(log_coeff + exponent).sum()\n",
    "    else:\n",
    "        return -(log_coeff + exponent)\n",
    "\n",
    "\n",
    "def get_kl_divergence(weights, prior, varpost):\n",
    "    prior_loglik = prior.loglik(weights)\n",
    "    \n",
    "    varpost_loglik = varpost.loglik(weights)\n",
    "    varpost_lik = varpost_loglik.exp()#exp func: y=e^x\n",
    "    \n",
    "    return (varpost_lik*(varpost_loglik - prior_loglik)).sum()\n",
    "\n",
    "\n",
    "class gaussian:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def loglik(self, weights):\n",
    "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
    "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
    "        \n",
    "        return (exponent + log_coeff).sum()\n",
    "class BayesLinear_Normalq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, prior):\n",
    "        super(BayesLinear_Normalq, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.prior = prior\n",
    "        \n",
    "        scale = (2/self.input_dim)**0.5\n",
    "        rho_init = np.log(np.exp((2/self.input_dim)**0.5) - 1)\n",
    "        self.weight_mus = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
    "        self.weight_rhos = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-3, -3))\n",
    "        \n",
    "        self.bias_mus = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-0.01, 0.01))\n",
    "        self.bias_rhos = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-4, -3))\n",
    "        \n",
    "    def forward(self, x, sample = True):\n",
    "        \n",
    "        if sample:\n",
    "            # sample gaussian noise for each weight and each bias\n",
    "            weight_epsilons = Variable(self.weight_mus.data.new(self.weight_mus.size()).normal_())\n",
    "            bias_epsilons =  Variable(self.bias_mus.data.new(self.bias_mus.size()).normal_())\n",
    "            \n",
    "            # calculate the weight and bias stds from the rho parameters\n",
    "            weight_stds = torch.log(1 + torch.exp(self.weight_rhos))\n",
    "            bias_stds = torch.log(1 + torch.exp(self.bias_rhos))\n",
    "            \n",
    "            # calculate samples from the posterior from the sampled noise and mus/stds\n",
    "            weight_sample = self.weight_mus + weight_epsilons*weight_stds\n",
    "            bias_sample = self.bias_mus + bias_epsilons*bias_stds\n",
    "            \n",
    "            #torch.cuda.synchronize()\n",
    "            output = torch.mm(x, weight_sample) + bias_sample\n",
    "            \n",
    "            # computing the KL loss term\n",
    "            prior_cov, varpost_cov = self.prior.sigma**2, weight_stds**2\n",
    "            KL_loss = 0.5*(torch.log(prior_cov/varpost_cov)).sum() - 0.5*weight_stds.numel()\n",
    "            KL_loss = KL_loss + 0.5*(varpost_cov/prior_cov).sum()\n",
    "            KL_loss = KL_loss + 0.5*((self.weight_mus - self.prior.mu)**2/prior_cov).sum()\n",
    "            \n",
    "            prior_cov, varpost_cov = self.prior.sigma**2, bias_stds**2\n",
    "            KL_loss = KL_loss + 0.5*(torch.log(prior_cov/varpost_cov)).sum() - 0.5*bias_stds.numel()\n",
    "            KL_loss = KL_loss + 0.5*(varpost_cov/prior_cov).sum()\n",
    "            KL_loss = KL_loss + 0.5*((self.bias_mus - self.prior.mu)**2/prior_cov).sum()\n",
    "            \n",
    "            return output, KL_loss\n",
    "        \n",
    "        else:\n",
    "            output = torch.mm(x, self.weight_mus) + self.bias_mus\n",
    "            return output, KL_loss\n",
    "        \n",
    "    def sample_layer(self, no_samples):\n",
    "        all_samples = []\n",
    "        for i in range(no_samples):\n",
    "            # sample gaussian noise for each weight and each bias\n",
    "            weight_epsilons = Variable(self.weight_mus.data.new(self.weight_mus.size()).normal_())\n",
    "            \n",
    "            # calculate the weight and bias stds from the rho parameters\n",
    "            weight_stds = torch.log(1 + torch.exp(self.weight_rhos))\n",
    "            \n",
    "            # calculate samples from the posterior from the sampled noise and mus/stds\n",
    "            weight_sample = self.weight_mus + weight_epsilons*weight_stds\n",
    "            \n",
    "            all_samples += weight_sample.view(-1).cpu().data.numpy().tolist()\n",
    "            \n",
    "        return all_samples\n",
    "class BBP_Heteroscedastic_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_units):\n",
    "        super(BBP_Heteroscedastic_Model, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # network with two hidden and one output layer\n",
    "        self.layer1 = BayesLinear_Normalq(input_dim, num_units, gaussian(0, 1))\n",
    "        self.layer2 = BayesLinear_Normalq(num_units, 2*output_dim, gaussian(0, 1))\n",
    "        \n",
    "        # activation to be used between hidden layers\n",
    "        #self.activation = nn.ReLU(inplace = True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        KL_loss_total = 0\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        x, KL_loss = self.layer1(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        #x = self.activation(x)\n",
    "        \n",
    "        x, KL_loss = self.layer2(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        \n",
    "        return x, KL_loss_total\n",
    "class BBP_Heteroscedastic_Model_Wrapper:\n",
    "    def __init__(self, network, learn_rate, batch_size, no_batches):\n",
    "        \n",
    "        self.learn_rate = learn_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.no_batches = no_batches\n",
    "        \n",
    "        self.network = network\n",
    "        self.network.cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = self.learn_rate)\n",
    "        self.loss_func = log_gaussian_loss\n",
    "    \n",
    "    def fit(self, x, y, no_samples):\n",
    "        \n",
    "        x, y = to_variable(var=(x, y), cuda=True)\n",
    "        \n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "        fit_loss_total = 0\n",
    "        \n",
    "        for i in range(no_samples):\n",
    "            output, KL_loss_total = self.network(x)\n",
    "            \n",
    "            # calculate fit loss based on mean and standard deviation of output\n",
    "            fit_loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1) \n",
    "            fit_loss_total = fit_loss_total + fit_loss\n",
    "        \n",
    "        KL_loss_total = KL_loss_total/self.no_batches\n",
    "        total_loss = (fit_loss_total + KL_loss_total)/(no_samples*x.shape[0])\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return fit_loss_total/no_samples, KL_loss_total\n",
    "    \n",
    "    def get_loss_and_rmse(self, x, y, no_samples):\n",
    "        x, y = to_variable(var=(x, y), cuda=True)\n",
    "        \n",
    "        means, stds = [], []\n",
    "        for i in range(no_samples):\n",
    "            output, KL_loss_total = self.network(x)\n",
    "            means.append(output[:, :1, None])\n",
    "            stds.append(output[:, 1:, None].exp())\n",
    "            \n",
    "        means, stds = torch.cat(means, 2), torch.cat(stds, 2)\n",
    "        mean = means.mean(dim=2)\n",
    "        std = (means.var(dim=2) + stds.mean(dim=2)**2)**0.5\n",
    "            \n",
    "        # calculate fit loss based on mean and standard deviation of output\n",
    "        logliks = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1, sum_reduce=False)\n",
    "        rmse = float((((mean - y)**2).mean()**0.5).cpu().data)\n",
    "\n",
    "        return logliks, rmse\n",
    "class BBP_Heteroscedastic_Model_eICU(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_units):\n",
    "        super(BBP_Heteroscedastic_Model_eICU, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # network with two hidden and one output layer\n",
    "        self.layer1 = BayesLinear_Normalq(input_dim, num_units[0], gaussian(0, 1))\n",
    "        self.layer2 = BayesLinear_Normalq(num_units[0], num_units[1], gaussian(0, 1))\n",
    "        self.layer3 = BayesLinear_Normalq(num_units[1], 2*output_dim, gaussian(0, 1))\n",
    "        \n",
    "        # activation to be used between hidden layers\n",
    "        #self.activation = nn.ReLU(inplace = True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        KL_loss_total = 0\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        \n",
    "        x, KL_loss = self.layer1(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        #x = self.activation(x)\n",
    "        \n",
    "        x, KL_loss = self.layer2(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        \n",
    "        return x, KL_loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of trainset is : 108988,53\n",
      "FOLD 0:\n",
      "Epoch: 001/100, Train loglik = -13.322, Val loglik = -12.972, Train RMSE = 4.928, Val RMSE = 4.967\n",
      "Epoch: 011/100, Train loglik = -2.812, Val loglik = -2.871, Train RMSE = 4.277, Val RMSE = 4.256\n",
      "Epoch: 021/100, Train loglik = -3.361, Val loglik = -3.522, Train RMSE = 3.860, Val RMSE = 3.885\n",
      "Epoch: 031/100, Train loglik = -2.886, Val loglik = -3.102, Train RMSE = 3.840, Val RMSE = 3.875\n",
      "Epoch: 041/100, Train loglik = -2.669, Val loglik = -2.694, Train RMSE = 3.892, Val RMSE = 3.946\n",
      "Epoch: 051/100, Train loglik = -2.746, Val loglik = -2.666, Train RMSE = 3.853, Val RMSE = 3.900\n",
      "Epoch: 061/100, Train loglik = -2.627, Val loglik = -2.641, Train RMSE = 3.797, Val RMSE = 3.824\n",
      "Epoch: 071/100, Train loglik = -2.630, Val loglik = -2.680, Train RMSE = 3.785, Val RMSE = 3.811\n",
      "Epoch: 081/100, Train loglik = -2.587, Val loglik = -2.623, Train RMSE = 3.788, Val RMSE = 3.820\n",
      "Epoch: 091/100, Train loglik = -2.668, Val loglik = -2.626, Train RMSE = 3.785, Val RMSE = 3.818\n",
      "Epoch: 100/100, Train loglik = -2.618, Val loglik = -2.636, Train RMSE = 3.782, Val RMSE = 3.811\n",
      "Train log. lik. = -3.195 +/-  1.707\n",
      "Test  log. lik. = -3.266 +/-  1.760\n",
      "Train RMSE      =  3.924 +/-  0.273\n",
      "Test  RMSE      =  3.955 +/-  0.272\n",
      "FOLD 1:\n",
      "Epoch: 001/100, Train loglik = -12.856, Val loglik = -12.792, Train RMSE = 4.941, Val RMSE = 4.920\n",
      "Epoch: 011/100, Train loglik = -2.753, Val loglik = -2.776, Train RMSE = 4.271, Val RMSE = 4.189\n",
      "Epoch: 021/100, Train loglik = -3.536, Val loglik = -3.495, Train RMSE = 3.860, Val RMSE = 3.818\n",
      "Epoch: 031/100, Train loglik = -2.878, Val loglik = -2.829, Train RMSE = 3.850, Val RMSE = 3.806\n",
      "Epoch: 041/100, Train loglik = -2.721, Val loglik = -2.701, Train RMSE = 3.899, Val RMSE = 3.869\n",
      "Epoch: 051/100, Train loglik = -2.618, Val loglik = -2.634, Train RMSE = 3.835, Val RMSE = 3.778\n",
      "Epoch: 061/100, Train loglik = -2.593, Val loglik = -2.629, Train RMSE = 3.799, Val RMSE = 3.750\n",
      "Epoch: 071/100, Train loglik = -2.775, Val loglik = -2.659, Train RMSE = 3.796, Val RMSE = 3.751\n",
      "Epoch: 081/100, Train loglik = -2.610, Val loglik = -2.657, Train RMSE = 3.795, Val RMSE = 3.755\n",
      "Epoch: 091/100, Train loglik = -2.601, Val loglik = -2.667, Train RMSE = 3.791, Val RMSE = 3.751\n",
      "Epoch: 100/100, Train loglik = -2.583, Val loglik = -2.658, Train RMSE = 3.785, Val RMSE = 3.746\n",
      "Train log. lik. = -3.196 +/-  1.700\n",
      "Test  log. lik. = -3.247 +/-  1.744\n",
      "Train RMSE      =  3.926 +/-  0.274\n",
      "Test  RMSE      =  3.922 +/-  0.279\n",
      "FOLD 2:\n",
      "Epoch: 001/100, Train loglik = -12.728, Val loglik = -11.972, Train RMSE = 4.968, Val RMSE = 4.783\n",
      "Epoch: 011/100, Train loglik = -2.831, Val loglik = -2.855, Train RMSE = 4.301, Val RMSE = 4.108\n",
      "Epoch: 021/100, Train loglik = -3.202, Val loglik = -3.251, Train RMSE = 3.882, Val RMSE = 3.689\n",
      "Epoch: 031/100, Train loglik = -2.886, Val loglik = -2.908, Train RMSE = 3.869, Val RMSE = 3.660\n",
      "Epoch: 041/100, Train loglik = -2.656, Val loglik = -2.652, Train RMSE = 3.928, Val RMSE = 3.723\n",
      "Epoch: 051/100, Train loglik = -2.642, Val loglik = -2.568, Train RMSE = 3.906, Val RMSE = 3.688\n",
      "Epoch: 061/100, Train loglik = -2.653, Val loglik = -2.542, Train RMSE = 3.826, Val RMSE = 3.601\n",
      "Epoch: 071/100, Train loglik = -2.639, Val loglik = -2.563, Train RMSE = 3.810, Val RMSE = 3.589\n",
      "Epoch: 081/100, Train loglik = -2.614, Val loglik = -2.570, Train RMSE = 3.812, Val RMSE = 3.588\n",
      "Epoch: 091/100, Train loglik = -2.615, Val loglik = -2.564, Train RMSE = 3.810, Val RMSE = 3.585\n",
      "Epoch: 100/100, Train loglik = -2.624, Val loglik = -2.566, Train RMSE = 3.803, Val RMSE = 3.583\n",
      "Train log. lik. = -3.210 +/-  1.716\n",
      "Test  log. lik. = -3.225 +/-  1.736\n",
      "Train RMSE      =  3.937 +/-  0.276\n",
      "Test  RMSE      =  3.862 +/-  0.296\n",
      "FOLD 3:\n",
      "Epoch: 001/100, Train loglik = -13.571, Val loglik = -11.303, Train RMSE = 4.977, Val RMSE = 4.650\n",
      "Epoch: 011/100, Train loglik = -3.310, Val loglik = -3.009, Train RMSE = 4.453, Val RMSE = 4.153\n",
      "Epoch: 021/100, Train loglik = -3.193, Val loglik = -3.240, Train RMSE = 3.943, Val RMSE = 3.598\n",
      "Epoch: 031/100, Train loglik = -2.899, Val loglik = -2.968, Train RMSE = 3.888, Val RMSE = 3.550\n",
      "Epoch: 041/100, Train loglik = -2.636, Val loglik = -2.584, Train RMSE = 3.966, Val RMSE = 3.627\n",
      "Epoch: 051/100, Train loglik = -2.638, Val loglik = -2.593, Train RMSE = 3.904, Val RMSE = 3.566\n",
      "Epoch: 061/100, Train loglik = -2.651, Val loglik = -2.684, Train RMSE = 3.835, Val RMSE = 3.509\n",
      "Epoch: 071/100, Train loglik = -2.622, Val loglik = -2.639, Train RMSE = 3.828, Val RMSE = 3.504\n",
      "Epoch: 081/100, Train loglik = -2.715, Val loglik = -2.541, Train RMSE = 3.825, Val RMSE = 3.508\n",
      "Epoch: 091/100, Train loglik = -2.600, Val loglik = -2.544, Train RMSE = 3.820, Val RMSE = 3.499\n",
      "Epoch: 100/100, Train loglik = -2.612, Val loglik = -2.543, Train RMSE = 3.817, Val RMSE = 3.497\n",
      "Train log. lik. = -3.237 +/-  1.774\n",
      "Test  log. lik. = -3.226 +/-  1.744\n",
      "Train RMSE      =  3.951 +/-  0.284\n",
      "Test  RMSE      =  3.812 +/-  0.311\n",
      "FOLD 4:\n",
      "Epoch: 001/100, Train loglik = -11.731, Val loglik = -12.615, Train RMSE = 4.940, Val RMSE = 4.957\n",
      "Epoch: 011/100, Train loglik = -2.811, Val loglik = -2.726, Train RMSE = 4.275, Val RMSE = 4.255\n",
      "Epoch: 021/100, Train loglik = -3.538, Val loglik = -3.607, Train RMSE = 3.868, Val RMSE = 3.844\n",
      "Epoch: 031/100, Train loglik = -2.929, Val loglik = -2.779, Train RMSE = 3.843, Val RMSE = 3.837\n",
      "Epoch: 041/100, Train loglik = -2.723, Val loglik = -2.665, Train RMSE = 3.904, Val RMSE = 3.895\n",
      "Epoch: 051/100, Train loglik = -2.711, Val loglik = -2.599, Train RMSE = 3.833, Val RMSE = 3.822\n",
      "Epoch: 061/100, Train loglik = -2.688, Val loglik = -2.627, Train RMSE = 3.798, Val RMSE = 3.780\n",
      "Epoch: 071/100, Train loglik = -2.631, Val loglik = -2.667, Train RMSE = 3.799, Val RMSE = 3.780\n",
      "Epoch: 081/100, Train loglik = -2.614, Val loglik = -2.592, Train RMSE = 3.798, Val RMSE = 3.783\n",
      "Epoch: 091/100, Train loglik = -2.595, Val loglik = -2.573, Train RMSE = 3.792, Val RMSE = 3.773\n",
      "Epoch: 100/100, Train loglik = -2.612, Val loglik = -2.594, Train RMSE = 3.786, Val RMSE = 3.770\n",
      "Train log. lik. = -3.237 +/-  1.781\n",
      "Test  log. lik. = -3.230 +/-  1.762\n",
      "Train RMSE      =  3.946 +/-  0.283\n",
      "Test  RMSE      =  3.833 +/-  0.309\n",
      "FOLD 5:\n",
      "Epoch: 001/100, Train loglik = -13.157, Val loglik = -13.523, Train RMSE = 4.954, Val RMSE = 4.846\n",
      "Epoch: 011/100, Train loglik = -2.885, Val loglik = -2.899, Train RMSE = 4.300, Val RMSE = 4.212\n",
      "Epoch: 021/100, Train loglik = -3.669, Val loglik = -3.246, Train RMSE = 3.885, Val RMSE = 3.789\n",
      "Epoch: 031/100, Train loglik = -2.908, Val loglik = -2.876, Train RMSE = 3.874, Val RMSE = 3.763\n",
      "Epoch: 041/100, Train loglik = -2.678, Val loglik = -2.664, Train RMSE = 3.951, Val RMSE = 3.860\n",
      "Epoch: 051/100, Train loglik = -2.617, Val loglik = -2.566, Train RMSE = 3.872, Val RMSE = 3.764\n",
      "Epoch: 061/100, Train loglik = -2.623, Val loglik = -2.588, Train RMSE = 3.807, Val RMSE = 3.714\n",
      "Epoch: 071/100, Train loglik = -2.619, Val loglik = -2.583, Train RMSE = 3.808, Val RMSE = 3.708\n",
      "Epoch: 081/100, Train loglik = -2.594, Val loglik = -2.544, Train RMSE = 3.813, Val RMSE = 3.711\n",
      "Epoch: 091/100, Train loglik = -2.614, Val loglik = -2.538, Train RMSE = 3.802, Val RMSE = 3.706\n",
      "Epoch: 100/100, Train loglik = -2.625, Val loglik = -2.541, Train RMSE = 3.797, Val RMSE = 3.703\n",
      "Train log. lik. = -3.240 +/-  1.802\n",
      "Test  log. lik. = -3.223 +/-  1.764\n",
      "Train RMSE      =  3.948 +/-  0.282\n",
      "Test  RMSE      =  3.836 +/-  0.304\n",
      "FOLD 6:\n",
      "Epoch: 001/100, Train loglik = -13.231, Val loglik = -13.056, Train RMSE = 4.971, Val RMSE = 4.824\n",
      "Epoch: 011/100, Train loglik = -2.741, Val loglik = -2.791, Train RMSE = 4.325, Val RMSE = 4.116\n",
      "Epoch: 021/100, Train loglik = -3.557, Val loglik = -3.296, Train RMSE = 3.873, Val RMSE = 3.692\n",
      "Epoch: 031/100, Train loglik = -3.014, Val loglik = -2.898, Train RMSE = 3.858, Val RMSE = 3.673\n",
      "Epoch: 041/100, Train loglik = -2.760, Val loglik = -2.684, Train RMSE = 3.936, Val RMSE = 3.745\n",
      "Epoch: 051/100, Train loglik = -2.629, Val loglik = -2.556, Train RMSE = 3.863, Val RMSE = 3.671\n",
      "Epoch: 061/100, Train loglik = -2.649, Val loglik = -2.637, Train RMSE = 3.810, Val RMSE = 3.618\n",
      "Epoch: 071/100, Train loglik = -2.658, Val loglik = -2.556, Train RMSE = 3.804, Val RMSE = 3.611\n",
      "Epoch: 081/100, Train loglik = -2.602, Val loglik = -2.558, Train RMSE = 3.809, Val RMSE = 3.616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 091/100, Train loglik = -2.623, Val loglik = -2.545, Train RMSE = 3.804, Val RMSE = 3.608\n",
      "Epoch: 100/100, Train loglik = -2.596, Val loglik = -2.555, Train RMSE = 3.801, Val RMSE = 3.606\n",
      "Train log. lik. = -3.253 +/-  1.837\n",
      "Test  log. lik. = -3.228 +/-  1.787\n",
      "Train RMSE      =  3.949 +/-  0.284\n",
      "Test  RMSE      =  3.827 +/-  0.305\n",
      "FOLD 7:\n",
      "Epoch: 001/100, Train loglik = -13.234, Val loglik = -12.602, Train RMSE = 4.942, Val RMSE = 4.912\n",
      "Epoch: 011/100, Train loglik = -2.832, Val loglik = -2.765, Train RMSE = 4.218, Val RMSE = 4.218\n",
      "Epoch: 021/100, Train loglik = -3.185, Val loglik = -3.458, Train RMSE = 3.863, Val RMSE = 3.802\n",
      "Epoch: 031/100, Train loglik = -2.881, Val loglik = -2.995, Train RMSE = 3.864, Val RMSE = 3.802\n",
      "Epoch: 041/100, Train loglik = -2.638, Val loglik = -2.720, Train RMSE = 3.934, Val RMSE = 3.875\n",
      "Epoch: 051/100, Train loglik = -2.644, Val loglik = -2.704, Train RMSE = 3.860, Val RMSE = 3.802\n",
      "Epoch: 061/100, Train loglik = -2.630, Val loglik = -2.669, Train RMSE = 3.795, Val RMSE = 3.732\n",
      "Epoch: 071/100, Train loglik = -2.613, Val loglik = -2.666, Train RMSE = 3.791, Val RMSE = 3.728\n",
      "Epoch: 081/100, Train loglik = -2.591, Val loglik = -2.653, Train RMSE = 3.792, Val RMSE = 3.735\n",
      "Epoch: 091/100, Train loglik = -2.647, Val loglik = -2.718, Train RMSE = 3.793, Val RMSE = 3.727\n",
      "Epoch: 100/100, Train loglik = -2.609, Val loglik = -2.659, Train RMSE = 3.790, Val RMSE = 3.726\n",
      "Train log. lik. = -3.243 +/-  1.817\n",
      "Test  log. lik. = -3.229 +/-  1.773\n",
      "Train RMSE      =  3.947 +/-  0.282\n",
      "Test  RMSE      =  3.833 +/-  0.303\n",
      "FOLD 8:\n",
      "Epoch: 001/100, Train loglik = -13.078, Val loglik = -15.837, Train RMSE = 4.888, Val RMSE = 5.465\n",
      "Epoch: 011/100, Train loglik = -2.806, Val loglik = -2.802, Train RMSE = 4.178, Val RMSE = 4.857\n",
      "Epoch: 021/100, Train loglik = -3.446, Val loglik = -3.435, Train RMSE = 3.787, Val RMSE = 4.477\n",
      "Epoch: 031/100, Train loglik = -2.966, Val loglik = -2.800, Train RMSE = 3.778, Val RMSE = 4.466\n",
      "Epoch: 041/100, Train loglik = -2.751, Val loglik = -2.820, Train RMSE = 3.846, Val RMSE = 4.546\n",
      "Epoch: 051/100, Train loglik = -2.639, Val loglik = -2.886, Train RMSE = 3.777, Val RMSE = 4.471\n",
      "Epoch: 061/100, Train loglik = -2.613, Val loglik = -2.708, Train RMSE = 3.713, Val RMSE = 4.430\n",
      "Epoch: 071/100, Train loglik = -2.638, Val loglik = -2.727, Train RMSE = 3.708, Val RMSE = 4.417\n",
      "Epoch: 081/100, Train loglik = -2.614, Val loglik = -2.668, Train RMSE = 3.708, Val RMSE = 4.412\n",
      "Epoch: 091/100, Train loglik = -2.595, Val loglik = -2.754, Train RMSE = 3.705, Val RMSE = 4.415\n",
      "Epoch: 100/100, Train loglik = -2.586, Val loglik = -2.680, Train RMSE = 3.701, Val RMSE = 4.407\n",
      "Train log. lik. = -3.237 +/-  1.803\n",
      "Test  log. lik. = -3.252 +/-  1.834\n",
      "Train RMSE      =  3.937 +/-  0.284\n",
      "Test  RMSE      =  3.912 +/-  0.373\n",
      "FOLD 9:\n",
      "Epoch: 001/100, Train loglik = -13.973, Val loglik = -13.922, Train RMSE = 4.933, Val RMSE = 5.056\n",
      "Epoch: 011/100, Train loglik = -2.738, Val loglik = -2.827, Train RMSE = 4.298, Val RMSE = 4.415\n",
      "Epoch: 021/100, Train loglik = -3.437, Val loglik = -3.661, Train RMSE = 3.856, Val RMSE = 4.013\n",
      "Epoch: 031/100, Train loglik = -3.042, Val loglik = -3.003, Train RMSE = 3.840, Val RMSE = 3.991\n",
      "Epoch: 041/100, Train loglik = -2.696, Val loglik = -2.743, Train RMSE = 3.919, Val RMSE = 4.078\n",
      "Epoch: 051/100, Train loglik = -2.639, Val loglik = -2.735, Train RMSE = 3.864, Val RMSE = 4.020\n",
      "Epoch: 061/100, Train loglik = -2.623, Val loglik = -2.690, Train RMSE = 3.780, Val RMSE = 3.945\n",
      "Epoch: 071/100, Train loglik = -2.609, Val loglik = -2.676, Train RMSE = 3.772, Val RMSE = 3.939\n",
      "Epoch: 081/100, Train loglik = -2.596, Val loglik = -2.652, Train RMSE = 3.774, Val RMSE = 3.933\n",
      "Epoch: 091/100, Train loglik = -2.594, Val loglik = -2.672, Train RMSE = 3.771, Val RMSE = 3.930\n",
      "Epoch: 100/100, Train loglik = -2.593, Val loglik = -2.649, Train RMSE = 3.769, Val RMSE = 3.927\n",
      "Train log. lik. = -3.237 +/-  1.799\n",
      "Test  log. lik. = -3.263 +/-  1.851\n",
      "Train RMSE      =  3.936 +/-  0.285\n",
      "Test  RMSE      =  3.929 +/-  0.368\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "trainset = pd.read_csv(\"/data/fjsdata/physionet/eICU-CRD/EMBC2020/trainset.csv\",sep=',',index_col=['patientunitstayid']) \n",
    "#min-max scale the continous features\n",
    "ss = MinMaxScaler()\n",
    "scale_features = ['ph', 'creatinine', 'albumin','diagnosis']\n",
    "trainset[scale_features] = ss.fit_transform(trainset[scale_features])\n",
    "print ('The shape of trainset is : %d,%d'%(trainset.shape[0],trainset.shape[1]))\n",
    "trainset = np.array(trainset)\n",
    "\n",
    "#model build and trained\n",
    "kf = KFold(n_splits=10)\n",
    "in_dim = trainset.shape[1]-1\n",
    "train_logliks, val_logliks = [], []\n",
    "train_rmses, val_rmses = [], []\n",
    "num_epochs = 100\n",
    "log_every=10\n",
    "best_net, best_loss = None, float('inf')\n",
    "\n",
    "for i, idx in enumerate(kf.split(trainset)):\n",
    "    print('FOLD %d:' % i)\n",
    "\n",
    "    train_index, val_index = idx\n",
    "\n",
    "    x_train, y_train = trainset[train_index, :in_dim], trainset[train_index, in_dim:]\n",
    "    x_val, y_val = trainset[val_index, :in_dim], trainset[val_index, in_dim:]\n",
    "\n",
    "    batch_size, nb_train = len(x_train), len(x_train)\n",
    "\n",
    "    net = BBP_Heteroscedastic_Model_Wrapper(network=BBP_Heteroscedastic_Model_eICU(input_dim=in_dim, output_dim=1, num_units=[16,4]),\n",
    "                                                learn_rate=1e-2, batch_size=batch_size, no_batches=1)\n",
    "\n",
    "    fit_loss_train = np.zeros(num_epochs)\n",
    "    KL_loss_train = np.zeros(num_epochs)\n",
    "    total_loss = np.zeros(num_epochs)\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        fit_loss, KL_loss = net.fit(x_train, y_train, no_samples = 20)\n",
    "        fit_loss_train[i] += fit_loss.cpu().data.numpy()\n",
    "        KL_loss_train[i] += KL_loss.cpu().data.numpy()\n",
    "\n",
    "        total_loss[i] = fit_loss_train[i] + KL_loss_train[i]\n",
    "\n",
    "        if fit_loss < best_loss:\n",
    "            best_loss = fit_loss\n",
    "            best_net = copy.deepcopy(net.network)\n",
    "\n",
    "        if i % log_every == 0 or i == num_epochs - 1:\n",
    "\n",
    "            train_losses, train_rmse = net.get_loss_and_rmse(x_train, y_train, 20)\n",
    "            val_losses, val_rmse = net.get_loss_and_rmse(x_val, y_val, 20)\n",
    "\n",
    "            print('Epoch: %s/%d, Train loglik = %.3f, Val loglik = %.3f, Train RMSE = %.3f, Val RMSE = %.3f' %\\\n",
    "                      (str(i+1).zfill(3), num_epochs, -train_losses.mean(),\n",
    "                       -val_losses.mean(), train_rmse, val_rmse))\n",
    "\n",
    "\n",
    "        train_losses, train_rmse = net.get_loss_and_rmse(x_train, y_train, 20)\n",
    "        val_losses, val_rmse = net.get_loss_and_rmse(x_val, y_val, 20)\n",
    "\n",
    "        train_logliks.append((train_losses.cpu().data.numpy().mean()))\n",
    "        val_logliks.append((val_losses.cpu().data.numpy().mean()))\n",
    "\n",
    "        train_rmses.append(train_rmse)\n",
    "        val_rmses.append(val_rmse)\n",
    "    print('Train log. lik. = %6.3f +/- %6.3f' % (-np.array(train_logliks).mean(), np.array(train_logliks).var()**0.5))\n",
    "    print('Test  log. lik. = %6.3f +/- %6.3f' % (-np.array(val_logliks).mean(), np.array(val_logliks).var()**0.5))\n",
    "    print('Train RMSE      = %6.3f +/- %6.3f' % (np.array(train_rmses).mean(), np.array(train_rmses).var()**0.5))\n",
    "    print('Test  RMSE      = %6.3f +/- %6.3f' % (np.array(val_rmses).mean(), np.array(val_rmses).var()**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of testset is : 27248,53\n",
      "MAE Score of BNN on eICU-CRD dataset is : 2.0062978810564114\n",
      "RMSE Score of BNN on eICU-CRD dataset is : 4.254167819908455\n",
      "R^2 Score of BNN on eICU-CRD dataset is : 0.08274125016091571\n",
      "EV Score of BNN on eICU-CRD dataset is : 0.08322966091803419\n"
     ]
    }
   ],
   "source": [
    "#load testset\n",
    "testset = pd.read_csv(\"/data/fjsdata/physionet/eICU-CRD/EMBC2020/testset.csv\",sep=',',index_col=['patientunitstayid'])\n",
    "testset[scale_features] = ss.fit_transform(testset[scale_features])\n",
    "print ('The shape of testset is : %d,%d'%(testset.shape[0],testset.shape[1]))\n",
    "testset = np.array(testset)\n",
    "x_test, y_test = testset[:, :in_dim], testset[:, in_dim:]\n",
    "x, y = to_variable(var=(x_test, y_test), cuda=True)\n",
    "        \n",
    "means, stds = [], []\n",
    "for i in range(100):#samples \n",
    "    output, KL_loss_total = best_net(x)\n",
    "    means.append(output[:, :1, None])\n",
    "    stds.append(output[:, 1:, None].exp())\n",
    "            \n",
    "means, stds = torch.cat(means, 2), torch.cat(stds, 2)\n",
    "mean = means.mean(dim=2)\n",
    "std = (means.var(dim=2) + stds.mean(dim=2)**2)**0.5\n",
    "            \n",
    "# calculate fit loss based on mean and standard deviation of output\n",
    "#rmse = float((((mean - y)**2).mean()**0.5).cpu().data)\n",
    "y_pred = mean.cpu().data.numpy()\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE Score of BNN on eICU-CRD dataset is :\", mae)  \n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE Score of BNN on eICU-CRD dataset is :\", rmse)  \n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R^2 Score of BNN on eICU-CRD dataset is :\", r2) \n",
    "ev = explained_variance_score(y_test, y_pred)\n",
    "print(\"EV Score of BNN on eICU-CRD dataset is :\", ev)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2.1 Bayesian Neural Networks with PRML\n",
    "PRML:https://github.com/ctgk/PRML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score,mean_squared_error\n",
    "import sys\n",
    "if \"PRML/\" not in sys.path:\n",
    "    sys.path.append(\"PRML/\")\n",
    "from prml import nn\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(nn.Network):\n",
    "\n",
    "    def __init__(self, shape):\n",
    "        super().__init__()\n",
    "        with self.set_parameter():\n",
    "            self.m = nn.zeros(shape)\n",
    "            self.s = nn.zeros(shape)\n",
    "\n",
    "    def __call__(self):\n",
    "        self.q = nn.Gaussian(self.m, nn.softplus(self.s) + 1e-8)\n",
    "        return self.q.draw()\n",
    "\n",
    "\n",
    "class BayesianNetwork(nn.Network):\n",
    "    \n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        with self.set_parameter():\n",
    "            self.qw1 = Gaussian((n_input, 16))\n",
    "            self.qb1 = Gaussian(16)\n",
    "            self.qw2 = Gaussian((16, 4))\n",
    "            self.qb2 = Gaussian(4)\n",
    "            self.qw3 = Gaussian((4, n_output))\n",
    "            self.qb3 = Gaussian(n_output)\n",
    "        self.posterior = [self.qw1, self.qb1, self.qw2, self.qb2, self.qw3, self.qb3]\n",
    "        self.prior = nn.Gaussian(0, 1)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = x @ self.qw1() + self.qb1()\n",
    "        h = h @ self.qw2() + self.qb2()\n",
    "        return nn.Gaussian(h @ self.qw3() + self.qb3(), 1)\n",
    "    \n",
    "    def kl(self):\n",
    "        kl = 0\n",
    "        for pos in self.posterior:\n",
    "            kl += nn.loss.kl_divergence(pos.q, self.prior).mean()\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of trainset is : 108988,53\n",
      "19900 / 20000"
     ]
    }
   ],
   "source": [
    "trainset = pd.read_csv(\"/data/fjsdata/physionet/eICU-CRD/EMBC2020/trainset.csv\",sep=',',index_col=['patientunitstayid']) \n",
    "#min-max scale the continous features\n",
    "ss = MinMaxScaler()\n",
    "scale_features = ['ph', 'creatinine', 'albumin','diagnosis']\n",
    "trainset[scale_features] = ss.fit_transform(trainset[scale_features])\n",
    "print ('The shape of trainset is : %d,%d'%(trainset.shape[0],trainset.shape[1]))\n",
    "X = trainset.drop(columns=[\"actualiculos\"], inplace=False)  #feature\n",
    "X = np.array(X)\n",
    "Y = trainset['actualiculos'].to_frame()#label\n",
    "model = BayesianNetwork(n_input=52, n_output=1)\n",
    "optimizer = nn.optimizer.Adam(model.parameter, 0.1)\n",
    "for i in range(20000):\n",
    "    model.clear()\n",
    "    py = model(X)\n",
    "    elbo = py.log_pdf(Y).mean(0).sum() - model.kl() / len(X)\n",
    "    optimizer.maximize(elbo)\n",
    "    #if i % 100 == 0:\n",
    "        #optimizer.learning_rate *= 0.9\n",
    "    if i % 100 == 0:\n",
    "        sys.stdout.write('\\r{} / {}'.format(i, 20000))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of testset is : 27248,53\n",
      "MAE Score of BNN on eICU-CRD dataset is : 1.888498947569551\n",
      "RMSE Score of BNN on eICU-CRD dataset is : 4.229033016664516\n",
      "R^2 Score of BNN on eICU-CRD dataset is : 0.09354806886582412\n",
      "EV Score of BNN on eICU-CRD dataset is : 0.09383464815454523\n"
     ]
    }
   ],
   "source": [
    "#testset\n",
    "teststet = pd.read_csv(\"/data/fjsdata/physionet/eICU-CRD/EMBC2020/testset.csv\",sep=',',index_col=['patientunitstayid'])\n",
    "teststet[scale_features] = ss.fit_transform(teststet[scale_features])\n",
    "print ('The shape of testset is : %d,%d'%(teststet.shape[0],teststet.shape[1]))\n",
    "X_test = teststet.drop(columns=[\"actualiculos\"], inplace=False)  #feature\n",
    "X_test = np.array(X_test)\n",
    "y_test = teststet['actualiculos'].to_frame()#label\n",
    "\n",
    "#prediciton 1\n",
    "#y_spl = []\n",
    "#for i in range(1000):#sample 2000\n",
    "#    y_spl.append(model(X_test).mean.value)\n",
    "#y_pred = np.array(y_spl).mean(axis=0)\n",
    "\n",
    "#prediciton 2\n",
    "#mae = []\n",
    "#rmse = []\n",
    "#r2 = []\n",
    "#ev = []\n",
    "#for i in range(2000):#sample 2000\n",
    "#    y_pred = model(X_test).mean.value\n",
    "#    mae.append(mean_absolute_error(y_test, y_pred))\n",
    "#    rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "#    r2 = r2_score(y_test, y_pred)\n",
    "#    ev = explained_variance_score(y_test, y_pred)\n",
    "#print(\"MAE Score of BNN on eICU-CRD dataset is :\", np.mean(mae))  \n",
    "#print(\"RMSE Score of BNN on eICU-CRD dataset is :\", np.mean(rmse))   \n",
    "#print(\"R^2 Score of BNN on eICU-CRD dataset is :\", np.mean(r2))  \n",
    "#print(\"EV Score of BNN on eICU-CRD dataset is :\", np.mean(ev)) \n",
    "\n",
    "#prediction 3\n",
    "mae = 5.0\n",
    "rmse = 5.0\n",
    "r2 = 0.0\n",
    "ev = 0.0\n",
    "for i in range(5000):#sample 2000, get best performance\n",
    "    y_pred = model(X_test).mean.value\n",
    "    t_mae = mean_absolute_error(y_test, y_pred)\n",
    "    if t_mae<mae: mae = t_mae\n",
    "    t_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "    if t_rmse<rmse: rmse = t_rmse\n",
    "    t_r2 = r2_score(y_test, y_pred)\n",
    "    if t_r2>r2: r2 = t_r2\n",
    "    t_ev = explained_variance_score(y_test, y_pred)\n",
    "    if t_ev>ev: ev = t_ev\n",
    "print(\"MAE Score of BNN on eICU-CRD dataset is :\",mae)\n",
    "print(\"RMSE Score of BNN on eICU-CRD dataset is :\", rmse)  \n",
    "print(\"R^2 Score of BNN on eICU-CRD dataset is :\", r2) \n",
    "print(\"EV Score of BNN on eICU-CRD dataset is :\", ev)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.2.DNNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of trainset is : 108988,53\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/data/tmpexec/eICU_DNN_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2bca9d1310>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x7f2bc73c2890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x7f2bc73c2890>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x7f2bc73c2890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x7f2bc73c2890>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x7f2bcb599690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x7f2bcb599690>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x7f2bcb599690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x7f2bcb599690>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2bc739c590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2bc739c590>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2bc739c590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2bc739c590>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2bc737f1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2bc737f1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2bc737f1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2bc737f1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2bc737f790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2bc737f790>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2bc737f790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2bc737f790>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/head.py:437: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /data/tmpexec/eICU_DNN_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 3806.9297, step = 0\n",
      "INFO:tensorflow:global_step/sec: 44.7083\n",
      "INFO:tensorflow:loss = 1874.3154, step = 100 (2.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.5747\n",
      "INFO:tensorflow:loss = 767.70624, step = 200 (2.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.9697\n",
      "INFO:tensorflow:loss = 1086.3562, step = 300 (2.032 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.7564\n",
      "INFO:tensorflow:loss = 2375.501, step = 400 (2.051 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.7717\n",
      "INFO:tensorflow:loss = 3548.195, step = 500 (2.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.7098\n",
      "INFO:tensorflow:loss = 2008.5344, step = 600 (2.113 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.6401\n",
      "INFO:tensorflow:loss = 1452.2113, step = 700 (2.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.2286\n",
      "INFO:tensorflow:loss = 1156.0033, step = 800 (2.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.9912\n",
      "INFO:tensorflow:loss = 2228.3352, step = 900 (2.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.5289\n",
      "INFO:tensorflow:loss = 2872.1562, step = 1000 (2.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.2933\n",
      "INFO:tensorflow:loss = 1008.8525, step = 1100 (2.071 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.9661\n",
      "INFO:tensorflow:loss = 958.19116, step = 1200 (1.988 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.42\n",
      "INFO:tensorflow:loss = 767.7645, step = 1300 (2.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.8957\n",
      "INFO:tensorflow:loss = 2688.4226, step = 1400 (2.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.7272\n",
      "INFO:tensorflow:loss = 1207.2203, step = 1500 (2.046 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.488\n",
      "INFO:tensorflow:loss = 1592.4209, step = 1600 (2.035 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.109\n",
      "INFO:tensorflow:loss = 886.73083, step = 1700 (2.022 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.6086\n",
      "INFO:tensorflow:loss = 1313.448, step = 1800 (2.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.3559\n",
      "INFO:tensorflow:loss = 2901.4087, step = 1900 (2.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.1397\n",
      "INFO:tensorflow:loss = 521.5415, step = 2000 (2.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.666\n",
      "INFO:tensorflow:loss = 2806.8862, step = 2100 (1.986 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.9966\n",
      "INFO:tensorflow:loss = 1322.1824, step = 2200 (2.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.1593\n",
      "INFO:tensorflow:loss = 1133.173, step = 2300 (2.042 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.6673\n",
      "INFO:tensorflow:loss = 1519.4755, step = 2400 (2.006 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.1186\n",
      "INFO:tensorflow:loss = 1351.797, step = 2500 (2.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.6848\n",
      "INFO:tensorflow:loss = 3384.9482, step = 2600 (2.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.7458\n",
      "INFO:tensorflow:loss = 469.39362, step = 2700 (2.044 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.6902\n",
      "INFO:tensorflow:loss = 858.4044, step = 2800 (2.012 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.1105\n",
      "INFO:tensorflow:loss = 1171.606, step = 2900 (2.037 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.8342\n",
      "INFO:tensorflow:loss = 912.9005, step = 3000 (2.061 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.4573\n",
      "INFO:tensorflow:loss = 1258.5148, step = 3100 (2.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.7053\n",
      "INFO:tensorflow:loss = 874.5059, step = 3200 (2.011 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.0797\n",
      "INFO:tensorflow:loss = 775.60693, step = 3300 (2.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.8559\n",
      "INFO:tensorflow:loss = 3811.7534, step = 3400 (2.044 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.7062\n",
      "INFO:tensorflow:loss = 1530.0884, step = 3500 (2.012 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.2187\n",
      "INFO:tensorflow:loss = 1417.4852, step = 3600 (2.031 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.101\n",
      "INFO:tensorflow:loss = 3944.2856, step = 3700 (2.037 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.9928\n",
      "INFO:tensorflow:loss = 2046.5808, step = 3800 (1.999 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.1026\n",
      "INFO:tensorflow:loss = 1174.8689, step = 3900 (1.999 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.1294\n",
      "INFO:tensorflow:loss = 1032.4127, step = 4000 (2.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.2179\n",
      "INFO:tensorflow:loss = 1377.6802, step = 4100 (2.039 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.8148\n",
      "INFO:tensorflow:loss = 1347.8363, step = 4200 (2.042 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.1053\n",
      "INFO:tensorflow:loss = 1062.4711, step = 4300 (2.049 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.3162\n",
      "INFO:tensorflow:loss = 1845.3899, step = 4400 (2.016 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.3101\n",
      "INFO:tensorflow:loss = 1095.4579, step = 4500 (2.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.9046\n",
      "INFO:tensorflow:loss = 1031.949, step = 4600 (2.003 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.3418\n",
      "INFO:tensorflow:loss = 1021.0753, step = 4700 (2.032 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.143\n",
      "INFO:tensorflow:loss = 1120.3693, step = 4800 (2.032 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.6135\n",
      "INFO:tensorflow:loss = 683.4392, step = 4900 (2.058 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.6289\n",
      "INFO:tensorflow:loss = 1596.1584, step = 5000 (2.044 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.7136\n",
      "INFO:tensorflow:loss = 4620.419, step = 5100 (2.012 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.0303\n",
      "INFO:tensorflow:loss = 725.0101, step = 5200 (2.042 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.4082\n",
      "INFO:tensorflow:loss = 4635.8076, step = 5300 (2.063 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.9288\n",
      "INFO:tensorflow:loss = 2548.1313, step = 5400 (2.043 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.143\n",
      "INFO:tensorflow:loss = 1686.7441, step = 5500 (2.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.2326\n",
      "INFO:tensorflow:loss = 3499.7134, step = 5600 (2.087 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.5288\n",
      "INFO:tensorflow:loss = 1093.7632, step = 5700 (2.050 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.2337\n",
      "INFO:tensorflow:loss = 1055.2798, step = 5800 (1.988 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.158\n",
      "INFO:tensorflow:loss = 1754.6228, step = 5900 (2.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.8349\n",
      "INFO:tensorflow:loss = 2322.7307, step = 6000 (2.135 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.5744\n",
      "INFO:tensorflow:loss = 1545.901, step = 6100 (2.016 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.6311\n",
      "INFO:tensorflow:loss = 590.3592, step = 6200 (2.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.4191\n",
      "INFO:tensorflow:loss = 1048.9902, step = 6300 (2.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.8502\n",
      "INFO:tensorflow:loss = 961.33716, step = 6400 (2.046 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.8981\n",
      "INFO:tensorflow:loss = 874.5084, step = 6500 (2.045 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.5569\n",
      "INFO:tensorflow:loss = 900.61536, step = 6600 (2.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.291\n",
      "INFO:tensorflow:loss = 1708.033, step = 6700 (2.073 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.8434\n",
      "INFO:tensorflow:loss = 1865.1979, step = 6800 (2.052 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.2219\n",
      "INFO:tensorflow:loss = 1772.448, step = 6900 (2.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.8267\n",
      "INFO:tensorflow:loss = 14134.46, step = 7000 (2.045 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.9474\n",
      "INFO:tensorflow:loss = 886.8845, step = 7100 (2.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.0131\n",
      "INFO:tensorflow:loss = 1334.6659, step = 7200 (2.068 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.1505\n",
      "INFO:tensorflow:loss = 1953.5636, step = 7300 (2.035 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1073.529, step = 7400 (2.042 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.6226\n",
      "INFO:tensorflow:loss = 1020.4949, step = 7500 (2.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.1992\n",
      "INFO:tensorflow:loss = 2009.9614, step = 7600 (2.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.7968\n",
      "INFO:tensorflow:loss = 2352.6812, step = 7700 (2.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.3291\n",
      "INFO:tensorflow:loss = 4675.4175, step = 7800 (2.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.3393\n",
      "INFO:tensorflow:loss = 1848.1088, step = 7900 (2.025 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.154\n",
      "INFO:tensorflow:loss = 2855.8977, step = 8000 (2.023 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.5672\n",
      "INFO:tensorflow:loss = 1449.3373, step = 8100 (2.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.5434\n",
      "INFO:tensorflow:loss = 2196.1973, step = 8200 (2.060 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.814\n",
      "INFO:tensorflow:loss = 2103.6677, step = 8300 (2.284 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.0644\n",
      "INFO:tensorflow:loss = 998.06006, step = 8400 (2.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.1758\n",
      "INFO:tensorflow:loss = 857.046, step = 8500 (2.033 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.5274\n",
      "INFO:tensorflow:loss = 1604.9713, step = 8600 (2.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.6363\n",
      "INFO:tensorflow:loss = 1499.3175, step = 8700 (2.012 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.2634\n",
      "INFO:tensorflow:loss = 2134.4446, step = 8800 (2.021 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.534\n",
      "INFO:tensorflow:loss = 2484.7378, step = 8900 (2.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.5009\n",
      "INFO:tensorflow:loss = 2810.4954, step = 9000 (2.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.6715\n",
      "INFO:tensorflow:loss = 1154.5505, step = 9100 (2.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.1233\n",
      "INFO:tensorflow:loss = 529.52026, step = 9200 (2.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.114\n",
      "INFO:tensorflow:loss = 905.6207, step = 9300 (2.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.0969\n",
      "INFO:tensorflow:loss = 1013.5347, step = 9400 (2.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.6812\n",
      "INFO:tensorflow:loss = 1749.6953, step = 9500 (2.054 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.6254\n",
      "INFO:tensorflow:loss = 3599.2617, step = 9600 (2.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.9522\n",
      "INFO:tensorflow:loss = 807.8513, step = 9700 (2.041 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.4767\n",
      "INFO:tensorflow:loss = 1499.3151, step = 9800 (1.981 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.9211\n",
      "INFO:tensorflow:loss = 1231.9667, step = 9900 (2.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.8903\n",
      "INFO:tensorflow:loss = 802.8368, step = 10000 (2.086 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.2975\n",
      "INFO:tensorflow:loss = 1241.2113, step = 10100 (2.028 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.9521\n",
      "INFO:tensorflow:loss = 1073.463, step = 10200 (1.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.2183\n",
      "INFO:tensorflow:loss = 1371.6589, step = 10300 (2.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.0602\n",
      "INFO:tensorflow:loss = 1237.7987, step = 10400 (1.997 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.4445\n",
      "INFO:tensorflow:loss = 1008.9654, step = 10500 (2.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.9226\n",
      "INFO:tensorflow:loss = 1019.0029, step = 10600 (2.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.3695\n",
      "INFO:tensorflow:loss = 1056.7904, step = 10700 (2.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.8316\n",
      "INFO:tensorflow:loss = 2598.7446, step = 10800 (2.048 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.3024\n",
      "INFO:tensorflow:loss = 1206.9702, step = 10900 (2.113 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.8765\n",
      "INFO:tensorflow:loss = 2081.8486, step = 11000 (2.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.7223\n",
      "INFO:tensorflow:loss = 679.7358, step = 11100 (2.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.2555\n",
      "INFO:tensorflow:loss = 1944.5979, step = 11200 (2.033 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.4748\n",
      "INFO:tensorflow:loss = 1341.9165, step = 11300 (2.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.4385\n",
      "INFO:tensorflow:loss = 701.8162, step = 11400 (2.023 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.837\n",
      "INFO:tensorflow:loss = 721.144, step = 11500 (2.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.7465\n",
      "INFO:tensorflow:loss = 2478.5154, step = 11600 (2.012 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.5738\n",
      "INFO:tensorflow:loss = 1326.3994, step = 11700 (2.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.3306\n",
      "INFO:tensorflow:loss = 1321.1117, step = 11800 (2.024 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.1286\n",
      "INFO:tensorflow:loss = 1152.1995, step = 11900 (2.034 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.4267\n",
      "INFO:tensorflow:loss = 1531.0813, step = 12000 (2.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.4638\n",
      "INFO:tensorflow:loss = 1076.9681, step = 12100 (2.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.7128\n",
      "INFO:tensorflow:loss = 5100.92, step = 12200 (2.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.9242\n",
      "INFO:tensorflow:loss = 1591.9392, step = 12300 (2.042 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.0155\n",
      "INFO:tensorflow:loss = 1858.0829, step = 12400 (2.012 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.4061\n",
      "INFO:tensorflow:loss = 903.5129, step = 12500 (1.970 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.2981\n",
      "INFO:tensorflow:loss = 586.3211, step = 12600 (2.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.1168\n",
      "INFO:tensorflow:loss = 2924.6138, step = 12700 (2.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.5723\n",
      "INFO:tensorflow:loss = 6591.0605, step = 12800 (2.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.295\n",
      "INFO:tensorflow:loss = 2340.1865, step = 12900 (2.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.7588\n",
      "INFO:tensorflow:loss = 2462.1204, step = 13000 (2.043 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.9888\n",
      "INFO:tensorflow:loss = 1107.5112, step = 13100 (2.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.4\n",
      "INFO:tensorflow:loss = 2169.3909, step = 13200 (2.038 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.3041\n",
      "INFO:tensorflow:loss = 1866.5072, step = 13300 (2.058 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.1948\n",
      "INFO:tensorflow:loss = 1299.1294, step = 13400 (2.038 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.1036\n",
      "INFO:tensorflow:loss = 3056.6223, step = 13500 (2.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.7605\n",
      "INFO:tensorflow:loss = 1728.0266, step = 13600 (2.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.9002\n",
      "INFO:tensorflow:loss = 3287.3545, step = 13700 (2.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.7999\n",
      "INFO:tensorflow:loss = 1306.1741, step = 13800 (2.019 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.9931\n",
      "INFO:tensorflow:loss = 1341.6511, step = 13900 (2.031 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.1927\n",
      "INFO:tensorflow:loss = 683.54443, step = 14000 (2.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.7167\n",
      "INFO:tensorflow:loss = 1606.3402, step = 14100 (2.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.6258\n",
      "INFO:tensorflow:loss = 1156.2982, step = 14200 (2.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.7628\n",
      "INFO:tensorflow:loss = 998.062, step = 14300 (2.051 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.1652\n",
      "INFO:tensorflow:loss = 1346.8757, step = 14400 (2.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.3447\n",
      "INFO:tensorflow:loss = 2334.9673, step = 14500 (2.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.0447\n",
      "INFO:tensorflow:loss = 2385.2747, step = 14600 (2.038 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.1096\n",
      "INFO:tensorflow:loss = 795.9088, step = 14700 (2.039 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.5388\n",
      "INFO:tensorflow:loss = 1315.2067, step = 14800 (2.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.4053\n",
      "INFO:tensorflow:loss = 1131.6603, step = 14900 (2.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.1431\n",
      "INFO:tensorflow:loss = 3544.9426, step = 15000 (2.032 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.962\n",
      "INFO:tensorflow:loss = 1249.6233, step = 15100 (2.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.2974\n",
      "INFO:tensorflow:loss = 3885.08, step = 15200 (2.016 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.9794\n",
      "INFO:tensorflow:loss = 3861.502, step = 15300 (2.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.581\n",
      "INFO:tensorflow:loss = 1706.916, step = 15400 (2.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.0173\n",
      "INFO:tensorflow:loss = 953.23737, step = 15500 (2.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.4571\n",
      "INFO:tensorflow:loss = 1458.162, step = 15600 (2.023 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 48.857\n",
      "INFO:tensorflow:loss = 1195.2837, step = 15700 (2.045 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.636\n",
      "INFO:tensorflow:loss = 1056.5255, step = 15800 (2.015 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.667\n",
      "INFO:tensorflow:loss = 743.67505, step = 15900 (1.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.5211\n",
      "INFO:tensorflow:loss = 1306.5133, step = 16000 (2.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.3836\n",
      "INFO:tensorflow:loss = 874.3021, step = 16100 (2.066 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.6347\n",
      "INFO:tensorflow:loss = 3718.0684, step = 16200 (2.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.1284\n",
      "INFO:tensorflow:loss = 6819.1445, step = 16300 (2.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.2156\n",
      "INFO:tensorflow:loss = 2772.75, step = 16400 (2.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.6477\n",
      "INFO:tensorflow:loss = 1134.8397, step = 16500 (2.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.3152\n",
      "INFO:tensorflow:loss = 1002.4207, step = 16600 (2.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.9063\n",
      "INFO:tensorflow:loss = 1468.3083, step = 16700 (2.332 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.9358\n",
      "INFO:tensorflow:loss = 1442.7754, step = 16800 (2.041 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.3769\n",
      "INFO:tensorflow:loss = 654.8093, step = 16900 (2.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.0275\n",
      "INFO:tensorflow:loss = 2976.1858, step = 17000 (2.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.5621\n",
      "INFO:tensorflow:loss = 1331.3516, step = 17100 (2.030 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.5538\n",
      "INFO:tensorflow:loss = 880.51575, step = 17200 (2.048 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.915\n",
      "INFO:tensorflow:loss = 2104.0415, step = 17300 (2.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.7554\n",
      "INFO:tensorflow:loss = 2477.5754, step = 17400 (2.040 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.9837\n",
      "INFO:tensorflow:loss = 500.90762, step = 17500 (2.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.3242\n",
      "INFO:tensorflow:loss = 1514.8403, step = 17600 (2.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.7663\n",
      "INFO:tensorflow:loss = 537.3591, step = 17700 (2.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.9353\n",
      "INFO:tensorflow:loss = 2943.597, step = 17800 (2.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.3368\n",
      "INFO:tensorflow:loss = 859.9, step = 17900 (2.033 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.7545\n",
      "INFO:tensorflow:loss = 1123.2507, step = 18000 (2.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.3552\n",
      "INFO:tensorflow:loss = 966.721, step = 18100 (2.059 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.0497\n",
      "INFO:tensorflow:loss = 1064.7078, step = 18200 (2.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.6631\n",
      "INFO:tensorflow:loss = 697.9616, step = 18300 (2.054 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.3977\n",
      "INFO:tensorflow:loss = 931.93945, step = 18400 (2.031 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.5473\n",
      "INFO:tensorflow:loss = 1517.3896, step = 18500 (2.053 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.5738\n",
      "INFO:tensorflow:loss = 1527.0615, step = 18600 (2.025 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.1269\n",
      "INFO:tensorflow:loss = 1199.6625, step = 18700 (2.028 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.2118\n",
      "INFO:tensorflow:loss = 10518.92, step = 18800 (2.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.2934\n",
      "INFO:tensorflow:loss = 1501.4485, step = 18900 (2.071 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.5266\n",
      "INFO:tensorflow:loss = 1104.4825, step = 19000 (2.061 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.0866\n",
      "INFO:tensorflow:loss = 3376.579, step = 19100 (2.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.139\n",
      "INFO:tensorflow:loss = 3065.9214, step = 19200 (1.995 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.3706\n",
      "INFO:tensorflow:loss = 695.21423, step = 19300 (2.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.9189\n",
      "INFO:tensorflow:loss = 934.5858, step = 19400 (2.045 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.3173\n",
      "INFO:tensorflow:loss = 1052.0027, step = 19500 (2.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.4471\n",
      "INFO:tensorflow:loss = 1452.5208, step = 19600 (2.064 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.1225\n",
      "INFO:tensorflow:loss = 1337.2698, step = 19700 (2.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.2951\n",
      "INFO:tensorflow:loss = 2289.9219, step = 19800 (2.028 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.4894\n",
      "INFO:tensorflow:loss = 1014.2743, step = 19900 (2.063 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 20000 into /data/tmpexec/eICU_DNN_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 825.3329.\n",
      "The shape of testset is : 27248,53\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Entity <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x7f2b8c341a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x7f2b8c341a90>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x7f2b8c341a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _DNNModel.call of <tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x7f2b8c341a90>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x7f2b8c341c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x7f2b8c341c10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x7f2b8c341c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DenseFeatures.call of <tensorflow.python.feature_column.feature_column_v2.DenseFeatures object at 0x7f2b8c341c10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2b8c344310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2b8c344310>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2b8c344310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2b8c344310>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2b8c344250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2b8c344250>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2b8c344250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2b8c344250>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2b8c344790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2b8c344790>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2b8c344790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2b8c344790>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /data/tmpexec/eICU_DNN_model/model.ckpt-20000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "MAE Score of DNN on eICU-CRD dataset is : 1.943248012066086\n",
      "RMSE Score of DNN on eICU-CRD dataset is : 4.204263548865531\n",
      "R^2 Score of DNN on eICU-CRD dataset is : 0.10413516058180439\n",
      "EV Score of DNN on eICU-CRD dataset is : 0.10528120293312215\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score,mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#1.2.1 load trainset\n",
    "trainset = pd.read_csv(\"/data/fjsdata/physionet/eICU-CRD/EMBC2020/trainset.csv\",sep=',',index_col=['patientunitstayid']) \n",
    "#min-max scale the continous features\n",
    "ss = MinMaxScaler()\n",
    "scale_features = ['ph', 'creatinine', 'albumin','diagnosis']\n",
    "trainset[scale_features] = ss.fit_transform(trainset[scale_features])\n",
    "print ('The shape of trainset is : %d,%d'%(trainset.shape[0],trainset.shape[1]))\n",
    "X = trainset.drop(columns=[\"actualiculos\"], inplace=False)  #feature\n",
    "Y = trainset['actualiculos']#label,\n",
    "\n",
    "#1.2.2 train model\n",
    "#Features filtered\n",
    "FEATURES = X.columns\n",
    "def get_input_fn(X,Y, num_epochs=None, shuffle=True):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "          x=pd.DataFrame({k: X[k].values for k in FEATURES}),\n",
    "          y=pd.Series(Y.values),\n",
    "          num_epochs=num_epochs,\n",
    "          shuffle=shuffle)\n",
    "#model\n",
    "feature_cols = [tf.feature_column.numeric_column(k) for k in FEATURES]\n",
    "regressor = tf.estimator.DNNRegressor(feature_columns=feature_cols, hidden_units=[16,4], model_dir=\"/data/tmpexec/eICU_DNN_model\")\n",
    "regressor.train(input_fn=get_input_fn(X,Y), steps=20000)\n",
    "\n",
    "#1.2.3 performance\n",
    "#testset\n",
    "teststet = pd.read_csv(\"/data/fjsdata/physionet/eICU-CRD/EMBC2020/testset.csv\",sep=',',index_col=['patientunitstayid'])\n",
    "teststet[scale_features] = ss.fit_transform(teststet[scale_features])\n",
    "print ('The shape of testset is : %d,%d'%(teststet.shape[0],teststet.shape[1]))\n",
    "X_test = teststet.drop(columns=[\"actualiculos\"], inplace=False)  #feature\n",
    "y_test = teststet['actualiculos']#label \n",
    "\n",
    "predictions = regressor.predict(input_fn=get_input_fn(X_test, y_test, num_epochs=1, shuffle=False))\n",
    "#predictions = list(p[\"predictions\"] for p in itertools.islice(y_pred, 6))\n",
    "#print(\"Predictions: {}\".format(str(predictions)))\n",
    "y_pred = []\n",
    "for it in list(predictions):\n",
    "    y_pred.append(it.get('predictions'))\n",
    "    \n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE Score of DNN on eICU-CRD dataset is :\", mae)  \n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE Score of DNN on eICU-CRD dataset is :\", rmse)  \n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R^2 Score of DNN on eICU-CRD dataset is :\", r2) \n",
    "ev = explained_variance_score(y_test, y_pred)\n",
    "print(\"EV Score of DNN on eICU-CRD dataset is :\", ev)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.1 DNN: TensorFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score,mean_squared_error\n",
    "from math import sqrt\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of trainset is : 108988,53\n",
      "100 / 109 : loss = 20.75131607055664\n",
      "100 / 109 : loss = 17.853490829467773736816406\n",
      "100 / 109 : loss = 16.69866561889648404284668\n",
      "100 / 109 : loss = 16.11250877380371219482422\n",
      "100 / 109 : loss = 15.768490791320820149536133\n",
      "100 / 109 : loss = 15.5463466644287113984375\n",
      "100 / 109 : loss = 15.392453193664552175598145\n",
      "100 / 109 : loss = 15.279600143432617690063477\n",
      "100 / 109 : loss = 15.1929569244384779085083\n",
      "100 / 109 : loss = 15.124031066894531168701172\n",
      "100 / 109 : loss = 15.067762374877931164855957\n",
      "100 / 109 : loss = 15.020967483520508096923828\n",
      "100 / 109 : loss = 14.981536865234375984863281\n",
      "100 / 109 : loss = 14.94800758361816474987793\n",
      "100 / 109 : loss = 14.919305801391602056152344\n",
      "100 / 109 : loss = 14.894625663757324432189941\n",
      "100 / 109 : loss = 14.873331069946289350646973\n",
      "100 / 109 : loss = 14.854907035827637370483398\n",
      "100 / 109 : loss = 14.838941574096687379760742\n",
      "100 / 109 : loss = 14.825090408325195661132812\n",
      "100 / 109 : loss = 14.813058853149414671386719\n",
      "100 / 109 : loss = 14.802598953247074300231934\n",
      "100 / 109 : loss = 14.793505668640137167419434\n",
      "100 / 109 : loss = 14.785596847534183854370117\n",
      "100 / 109 : loss = 14.778719902038574274780273\n",
      "100 / 109 : loss = 14.772740364074707720947266\n",
      "100 / 109 : loss = 14.767542839050293327026367\n",
      "100 / 109 : loss = 14.763028144836426161682129\n",
      "100 / 109 : loss = 14.759107589721685320739746\n",
      "100 / 109 : loss = 14.755701065063477344116211\n",
      "100 / 109 : loss = 14.752743721008349567260742\n",
      "100 / 109 : loss = 14.75017261505127246459961\n",
      "100 / 109 : loss = 14.747937202453613723022461\n",
      "100 / 109 : loss = 14.745992660522461334533691\n",
      "100 / 109 : loss = 14.74429512023925861541748\n",
      "100 / 109 : loss = 14.742813110351562048095703\n",
      "Mean loss in this epoch is: 14.0467529296875"
     ]
    }
   ],
   "source": [
    "#1.1.1 design model\n",
    "class TF_DNNRegressor_eICU:\n",
    "    def __init__(self, lr=0.001, dim=52, num_class=1, batchSize=1000):\n",
    "        #global parameters\n",
    "        self.lr = lr\n",
    "        self.dim = dim # dimensions of sample\n",
    "        self.num_class = num_class #output \n",
    "        self.hidden_layers = [16, 4]\n",
    "        #set network structure\n",
    "        self.add_placeholders()\n",
    "        self.add_weight()\n",
    "        self.add_model()\n",
    "        self.add_loss()\n",
    "        self.add_optimizer()\n",
    "        self.init_sess()\n",
    "        \n",
    "    def add_placeholders(self):    \n",
    "        self.X_input = tf.placeholder(\"float\", [None, self.dim])\n",
    "        self.Y_input = tf.placeholder(\"float\", [None, self.num_class])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)  \n",
    "    \n",
    "    def add_weight(self):\n",
    "        # Store layers weight & bias\n",
    "        #init_uniform = tf.random_uniform_initializer(minval=0, maxval=1, seed=None, dtype=tf.float32)\n",
    "        self.weights = {\n",
    "            'w1': tf.Variable(tf.random_normal([self.dim, self.hidden_layers[0]])),\n",
    "            'w2': tf.Variable(tf.random_normal([self.hidden_layers[0], self.hidden_layers[1]])),\n",
    "            #'w3': tf.Variable(tf.random_normal([self.hidden_layers[1], self.hidden_layers[2]])),\n",
    "            #'w4': tf.Variable(tf.random_normal([self.hidden_layers[2], self.hidden_layers[3]])),\n",
    "            'wout': tf.Variable(tf.random_normal([self.hidden_layers[1], self.num_class]))\n",
    "        }\n",
    "        self.biases = {\n",
    "            'b1': tf.Variable(tf.random_normal([self.hidden_layers[0]])),\n",
    "            'b2': tf.Variable(tf.random_normal([self.hidden_layers[1]])),\n",
    "            #'b3': tf.Variable(tf.random_normal([self.hidden_layers[2]])),\n",
    "            #'b4': tf.Variable(tf.random_normal([self.hidden_layers[3]])),\n",
    "            'bout': tf.Variable(tf.random_normal([self.num_class]))\n",
    "        }\n",
    "        \n",
    "    def add_model(self):\n",
    "        # Hidden fully connected layer with 1024 neurons\n",
    "        layer_1 =  tf.add(tf.matmul(self.X_input, self.weights['w1']), self.biases['b1']) \n",
    "        # Hidden fully connected layer with 256 neurons\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, self.weights['w2']), self.biases['b2']) \n",
    "        # Hidden fully connected layer with 128 neurons\n",
    "        #layer_3 =  tf.add(tf.matmul(layer_2, self.weights['w3']), self.biases['b3']) \n",
    "        # Hidden fully connected layer with 32 neurons \n",
    "        #layer_4 =  tf.add(tf.matmul(layer_3, self.weights['w4']), self.biases['b4']) \n",
    "        # Output fully connected layer with a neuron for each class\n",
    "        out_layer =tf.matmul(layer_2, self.weights['wout']) + self.biases['bout'] \n",
    "        self.Y_output =  out_layer\n",
    "    \n",
    "    def add_loss(self):\n",
    "         self.loss = tf.losses.mean_squared_error( self.Y_input , self.Y_output ) \n",
    "            \n",
    "    def add_optimizer(self):\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_step = optimizer.minimize(self.loss)\n",
    "        \n",
    "    def init_sess(self):\n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        self.config.allow_soft_placement = True\n",
    "        #self.config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "        self.sess = tf.Session(config=self.config)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#1.1.2 train model\n",
    "#load trainset\n",
    "trainset = pd.read_csv(\"/data/fjsdata/physionet/eICU-CRD/EMBC2020/trainset.csv\",sep=',',index_col=['patientunitstayid']) \n",
    "#min-max scale the continous features\n",
    "ss = MinMaxScaler()\n",
    "scale_features = ['ph', 'creatinine', 'albumin','diagnosis']\n",
    "trainset[scale_features] = ss.fit_transform(trainset[scale_features])\n",
    "print ('The shape of trainset is : %d,%d'%(trainset.shape[0],trainset.shape[1]))\n",
    "X = trainset.drop(columns=[\"actualiculos\"], inplace=False)  #feature\n",
    "Y = trainset['actualiculos'].to_frame()#label, from series to array\n",
    "#define model\n",
    "tf_model = TF_DNNRegressor_eICU()\n",
    "#set paramete\n",
    "verbose = 10\n",
    "batchSize=1000\n",
    "num_batches = X.shape[0] // batchSize + 1 \n",
    "pre_loss = 0.0\n",
    "while True:#convergence\n",
    "    losses = []\n",
    "    for i in range(num_batches):\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([X.shape[0], (i+1)*batchSize])\n",
    "        X_batch = X[min_idx: max_idx]\n",
    "        Y_batch = Y[min_idx: max_idx]\n",
    "        #_, tmp_loss, y_out = tf_model.sess.run([tf_model.train_step, tf_model.loss, tf_model.Y_output], \n",
    "        #                                 feed_dict={tf_model.X_input: X_batch,tf_model.Y_input: Y_batch})\n",
    "        _, tmp_loss,  = tf_model.sess.run([tf_model.train_step, tf_model.loss], \n",
    "                                         feed_dict={tf_model.X_input: X_batch,tf_model.Y_input: Y_batch, tf_model.keep_prob: 0.6})\n",
    "        losses.append(tmp_loss)\n",
    "        if verbose and i % verbose == 0:\n",
    "            sys.stdout.write('\\r{} / {} : loss = {}'.format(i, num_batches, np.mean(losses[-verbose:])))\n",
    "            sys.stdout.flush()\n",
    "    sys.stdout.write(\"\\nMean loss in this epoch is: {}\".format( np.mean(losses) ))\n",
    "    sys.stdout.flush()\n",
    "    #whether convergence\n",
    "    if abs( np.mean(losses) - pre_loss)<0.001:\n",
    "        break\n",
    "    else:\n",
    "        pre_loss = np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of testset is : 27248,53\n",
      "MAE Score of DNN on eICU-CRD dataset is : 2.002567524068769\n",
      "RMSE Score of DNN on eICU-CRD dataset is : 4.231178565719067\n",
      "R^2 Score of DNN on eICU-CRD dataset is : 0.09262808057486926\n",
      "EV Score of DNN on eICU-CRD dataset is : 0.09273067330108298\n"
     ]
    }
   ],
   "source": [
    "#1.1.3  prediction and evaluation\n",
    "#testset\n",
    "teststet = pd.read_csv(\"/data/fjsdata/physionet/eICU-CRD/EMBC2020/testset.csv\",sep=',',index_col=['patientunitstayid'])\n",
    "teststet[scale_features] = ss.fit_transform(teststet[scale_features])\n",
    "print ('The shape of testset is : %d,%d'%(teststet.shape[0],teststet.shape[1]))\n",
    "X_test = teststet.drop(columns=[\"actualiculos\"], inplace=False)  #feature\n",
    "y_test = teststet['actualiculos'].to_frame()#label \n",
    "#prediction\n",
    "y_pred = tf_model.sess.run(tf_model.Y_output, feed_dict={tf_model.X_input: X_test,tf_model.Y_input: y_test, tf_model.keep_prob: 1}) \n",
    "#y_pred = tf_model.sess.run(tf.nn.relu(y_pred))\n",
    "#np.set_printoptions(precision=4)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE Score of DNN on eICU-CRD dataset is :\", mae)  \n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE Score of DNN on eICU-CRD dataset is :\", rmse)  \n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R^2 Score of DNN on eICU-CRD dataset is :\", r2) \n",
    "ev = explained_variance_score(y_test, y_pred)\n",
    "print(\"EV Score of DNN on eICU-CRD dataset is :\", ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
